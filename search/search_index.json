{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Directorio de grabaciones de SRE \u00b6 Este directorio es creado con el objetivo de facilitar el acceso a grabaciones con alto valor de aprendizaje sobre diferentes temas de la tribu, como terraform, kubeternetes, administracion azure, linux, etc. Aqui se van registrando todas las grabaciones donde diferentes integrantes de la tribu explican un tema importante. Temas \u00b6 terraform - IaC Infraestructura como codigo de los diferentes ambientes de Siigo. kubernetes - Orquestacion de la infraestructura de siigo. microservicios - Estandar manejado .... linux / windows - Comandos del SO, gestion de servidores. datadog - Ver los logs, estadisticas, etc de los diferentes servicios de siigo. Siigo CLI - Como funciona la herramienta Siigo CLI , como contribuir. Azure - Administracion, gestion de permisos, accesos, recursos.","title":"Directorio de grabaciones de SRE"},{"location":"#directorio-de-grabaciones-de-sre","text":"Este directorio es creado con el objetivo de facilitar el acceso a grabaciones con alto valor de aprendizaje sobre diferentes temas de la tribu, como terraform, kubeternetes, administracion azure, linux, etc. Aqui se van registrando todas las grabaciones donde diferentes integrantes de la tribu explican un tema importante.","title":"Directorio de grabaciones de SRE"},{"location":"#temas","text":"terraform - IaC Infraestructura como codigo de los diferentes ambientes de Siigo. kubernetes - Orquestacion de la infraestructura de siigo. microservicios - Estandar manejado .... linux / windows - Comandos del SO, gestion de servidores. datadog - Ver los logs, estadisticas, etc de los diferentes servicios de siigo. Siigo CLI - Como funciona la herramienta Siigo CLI , como contribuir. Azure - Administracion, gestion de permisos, accesos, recursos.","title":"Temas"},{"location":"datadog/","text":"Datadog \u00b6 Un pequeno texto de lo que se hace con datadog en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo COnfiguraciond e Dashboard \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion Alertas \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion Manejo de Kubectl \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion # Ejemplo de creacion de pods kubectl get pods --namespace curso-namespace kubectl apply -f mongopod.yaml kubectl get pods --namespace curso-namespace kubectl describe pod mongo --namespace curso-namespace kubectl run custom-nginx --image = bitnami/nginx --restart = Never --namespace curso-namespace kubectl get pods --namespace curso-namespace kubectl get pod custom-nginx --namespace curso-namespace -o json kubectl get pod custom-nginx --namespace curso-namespace -o yaml kubectl exec -ti custom-nginx --namespace curso-namespace -- bash kubectl logs custom-nginx --namespace curso-namespace kubectl port-forward custom-nginx 8080 --namespace curso-namespace & curl localhost:8080 { \"key\" : \"value\" }","title":"Datadog"},{"location":"datadog/#datadog","text":"Un pequeno texto de lo que se hace con datadog en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo","title":"Datadog"},{"location":"datadog/#configuraciond-e-dashboard","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion","title":"COnfiguraciond e Dashboard"},{"location":"datadog/#breakpoints","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"datadog/#anotaciones","text":"Anotaciones importantes de la grabacion","title":"Anotaciones"},{"location":"datadog/#alertas","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion","title":"Alertas"},{"location":"datadog/#breakpoints_1","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"datadog/#anotaciones_1","text":"Anotaciones importantes de la grabacion","title":"Anotaciones"},{"location":"datadog/#manejo-de-kubectl","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion","title":"Manejo de Kubectl"},{"location":"datadog/#breakpoints_2","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"datadog/#anotaciones_2","text":"Anotaciones importantes de la grabacion # Ejemplo de creacion de pods kubectl get pods --namespace curso-namespace kubectl apply -f mongopod.yaml kubectl get pods --namespace curso-namespace kubectl describe pod mongo --namespace curso-namespace kubectl run custom-nginx --image = bitnami/nginx --restart = Never --namespace curso-namespace kubectl get pods --namespace curso-namespace kubectl get pod custom-nginx --namespace curso-namespace -o json kubectl get pod custom-nginx --namespace curso-namespace -o yaml kubectl exec -ti custom-nginx --namespace curso-namespace -- bash kubectl logs custom-nginx --namespace curso-namespace kubectl port-forward custom-nginx 8080 --namespace curso-namespace & curl localhost:8080 { \"key\" : \"value\" }","title":"Anotaciones"},{"location":"harness/","text":"Harness \u00b6 Siigo tiene el objetivo de tiener utiliza harness para provisionar Un pequeno texto de lo que se hace con harness en siigo Un pequeno texto de lo que se hace con harness en siigo Un pequeno texto de lo que se hace con harness en siigo Un pequeno texto de lo que se hace con harness en siigo Un pequeno texto de lo que se hace con harness en siigo Repositorio de harness Migrar el Cluster de QA MEx a SPOT \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Teams Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion Migrar el Cluster de QA MEx a SPOT \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Teams Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion Migrar el Cluster de QA MEx a SPOT \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Teams Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion Migrar el Cluster de QA MEx a SPOT \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Teams Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion Migrar el Cluster de QA MEx a SPOT \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Teams Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion","title":"Harness"},{"location":"harness/#harness","text":"Siigo tiene el objetivo de tiener utiliza harness para provisionar Un pequeno texto de lo que se hace con harness en siigo Un pequeno texto de lo que se hace con harness en siigo Un pequeno texto de lo que se hace con harness en siigo Un pequeno texto de lo que se hace con harness en siigo Un pequeno texto de lo que se hace con harness en siigo Repositorio de harness","title":"Harness"},{"location":"harness/#migrar-el-cluster-de-qa-mex-a-spot","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Teams","title":"Migrar el Cluster de QA MEx a SPOT"},{"location":"harness/#breakpoints","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"harness/#anotaciones","text":"Anotaciones importantes de la grabacion","title":"Anotaciones"},{"location":"harness/#migrar-el-cluster-de-qa-mex-a-spot_1","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Teams","title":"Migrar el Cluster de QA MEx a SPOT"},{"location":"harness/#breakpoints_1","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"harness/#anotaciones_1","text":"Anotaciones importantes de la grabacion","title":"Anotaciones"},{"location":"harness/#migrar-el-cluster-de-qa-mex-a-spot_2","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Teams","title":"Migrar el Cluster de QA MEx a SPOT"},{"location":"harness/#breakpoints_2","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"harness/#anotaciones_2","text":"Anotaciones importantes de la grabacion","title":"Anotaciones"},{"location":"harness/#migrar-el-cluster-de-qa-mex-a-spot_3","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Teams","title":"Migrar el Cluster de QA MEx a SPOT"},{"location":"harness/#breakpoints_3","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"harness/#anotaciones_3","text":"Anotaciones importantes de la grabacion","title":"Anotaciones"},{"location":"harness/#migrar-el-cluster-de-qa-mex-a-spot_4","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Teams","title":"Migrar el Cluster de QA MEx a SPOT"},{"location":"harness/#breakpoints_4","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"harness/#anotaciones_4","text":"Anotaciones importantes de la grabacion","title":"Anotaciones"},{"location":"kubernetes/","text":"Kubernetes \u00b6 Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Migrar el Cluster de QA MEx a SPOT \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion Kubernetes Tema 1 \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion Manejo de Kubectl \u00b6 Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion Breakpoints \u00b6 Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure Anotaciones \u00b6 Anotaciones importantes de la grabacion # Ejemplo de creacion de pods kubectl get pods --namespace curso-namespace kubectl apply -f mongopod.yaml kubectl get pods --namespace curso-namespace kubectl describe pod mongo --namespace curso-namespace kubectl run custom-nginx --image = bitnami/nginx --restart = Never --namespace curso-namespace kubectl get pods --namespace curso-namespace kubectl get pod custom-nginx --namespace curso-namespace -o json kubectl get pod custom-nginx --namespace curso-namespace -o yaml kubectl exec -ti custom-nginx --namespace curso-namespace -- bash kubectl logs custom-nginx --namespace curso-namespace kubectl port-forward custom-nginx 8080 --namespace curso-namespace & curl localhost:8080 { \"key\" : \"value\" }","title":"Kubernetes"},{"location":"kubernetes/#kubernetes","text":"Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo Un pequeno texto de lo que se hace con kubernetes en siigo","title":"Kubernetes"},{"location":"kubernetes/#migrar-el-cluster-de-qa-mex-a-spot","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion","title":"Migrar el Cluster de QA MEx a SPOT"},{"location":"kubernetes/#breakpoints","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"kubernetes/#anotaciones","text":"Anotaciones importantes de la grabacion","title":"Anotaciones"},{"location":"kubernetes/#kubernetes-tema-1","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion","title":"Kubernetes Tema 1"},{"location":"kubernetes/#breakpoints_1","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"kubernetes/#anotaciones_1","text":"Anotaciones importantes de la grabacion","title":"Anotaciones"},{"location":"kubernetes/#manejo-de-kubectl","text":"Migracion de clusters de QA Mex a SPOT, proceso detallado de como actualizar los clusters a la nueva version de el modulo de infraestructure-modules de Clusters con spot Url Reunion","title":"Manejo de Kubectl"},{"location":"kubernetes/#breakpoints_2","text":"Min 0 Ambientacion Min 15 Modificacion de terra Min 50 Spotins Min 120 Network en Azure","title":"Breakpoints"},{"location":"kubernetes/#anotaciones_2","text":"Anotaciones importantes de la grabacion # Ejemplo de creacion de pods kubectl get pods --namespace curso-namespace kubectl apply -f mongopod.yaml kubectl get pods --namespace curso-namespace kubectl describe pod mongo --namespace curso-namespace kubectl run custom-nginx --image = bitnami/nginx --restart = Never --namespace curso-namespace kubectl get pods --namespace curso-namespace kubectl get pod custom-nginx --namespace curso-namespace -o json kubectl get pod custom-nginx --namespace curso-namespace -o yaml kubectl exec -ti custom-nginx --namespace curso-namespace -- bash kubectl logs custom-nginx --namespace curso-namespace kubectl port-forward custom-nginx 8080 --namespace curso-namespace & curl localhost:8080 { \"key\" : \"value\" }","title":"Anotaciones"},{"location":"siigo%20billing/","text":"Siigo Costs \u00b6 La aplicaci\u00f3n tiene el objetivo de procesar y guardar los costos de infraestructura de Siigo SAS , de los diferentes clouds azure , aws , oracle , por medio de diferentes providers como rocket , azure costs management , aws cost , para poder generar m\u00e9tricas, indicadores, reportes, donde se puedan ejecutar an\u00e1lisis y de forma oportuna poder responder preguntas como: Donde y porque se incrementaron los costos este mes. Determinar que parte del incremento de costos es de nueva infra y de infra existente. Es el d\u00eda 7, 15, etc. del mes y quiero saber cuales son los recursos que incrementaron sus costos lo que lleva del mes. Cu\u00e1l ser\u00e1 el costo de x recursos el mes que viene(Predicciones avanzadas). Cu\u00e1l es el costo exacto de la nueva infraestructura desplegada este mes. Llevar un hist\u00f3rico de los costos tambi\u00e9n es algo muy valioso, ya que con esto podremos ver como cambian los costos de un recurso, grupo de recursos a lo largo del tiempo. Cu\u00e1l es el descuento exacto que aplic\u00f3 Rocket para cada tipo de recurso el mes que pas\u00f3. Diagrama de arquitectura \u00b6 La aplicaci\u00f3n se compone de 3 componentes principales: Siigo Resources Management : Tiene como objetivo bajar los recursos que est\u00e1n desplegados en los diferentes clouds(en este momento solo azure), ya que en los informes de costos se quiere obtener la informaci\u00f3n de cada recurso(tipo, regi\u00f3n, sku, etc.) como tambi\u00e9n los tags(owner, tribu, product, solution). Siigo Cost Processor : Tiene como objetivo bajar los costos de infraestructura de los recursos que est\u00e1n desplegados en los diferentes clouds, por medio de diferentes providers, como Rocket , Azure Cost Management , AWS Costs , para luego procesarlos, agruparlos y relacionarlos a los recursos que est\u00e1n guardados en la base de datos. El objetivo es bajar los costos en intervalos establecidos teniendo en cuenta las restricciones de cada proveedor, por ejemplo: cada mes, cada semana, cada 15 d\u00edas. Siigo Cost API : Es un API enfocada 100% a consultas sobre la info en la db, tiene como objetivo generar diferentes informes de costos, por suscripci\u00f3n, grupo de recursos, recursos, providers, etc, como tambi\u00e9n de recursos, en excel o json . POWER BI : Esta tecnolog\u00eda externa nos ayudar\u00e1 a analizar la informaci\u00f3n de una forma m\u00e1s sencilla, con gr\u00e1ficos, consultas complejas etc, power bi tiene las opciones de obtener datos de un API o una db, en este caso MongoDB. Siigo Resources Management \u00b6 Por el momento este componente solo baja NUEVOS RECURSOS , m\u00e1s adelante podr\u00eda ser capaz de determinar si un recurso ha sido ELIMINADO o ACTUALIZADO , y reflejarlo en la db. Id Factory : Este componente tiene el objetivo de generar id aleatorios para cada nuevo recurso o grupo de recursos o sub Subscription Fetcher : Este componente tiene el objetivo de consultar las suscripciones de azure a trav\u00e9s del api, procesarlas y retornarlas en formato json. Resource Groups Fetcher : Este componente tiene el objetivo de consultar los recursos de azure a trav\u00e9s del api, procesarlos y retornarlas en formato json. Resources Fetcher : Este componente tiene el objetivo de consultar los recursos de azure a trav\u00e9s del api, procesarlos y retornarlas en formato json. Resources From Costs Fetcher : Este componente existe debido a que no es posible obtener todos los recursos que existieron en un mes con los tres componentes anteriores, ya que el API Azure Resource Management de azure solo retorna los que existen en el momento que se hace la petici\u00f3n. Este componente extrae todos los recursos y grupos de recursos no existentes en la db que vienen en la respuesta de Azure Cost Management los procesa y retorna en en formato json. Cabe aclarar que usando l\u00f3gica difusa se determina cual es el Service Name para cada recurso, ya que en la respuesta de costos para un mismo recurso normalmente vienen registros con diferentes Services Names (Un problema de ambig\u00fcedad). Este proceso se pudo validar en una PoC comparando los resultados con el portal de Azure y se encontr\u00f3 que funciona correctamente para un grupo mediano de datos. Azure Fetcher Main : Este componente tiene el objetivo de llamar a los diferentes fetchers y posteriormente guardar la informaci\u00f3n en la base de datos. Siigo Costs Processor \u00b6 Este es un dise\u00f1o simplificado de este m\u00f3dulo, este componente consulta en este momento informaci\u00f3n de costos directamente de Azure y de Rocket , los procesa y guarda en la base de datos. Rocket Cost API : Este componente se comunica con el API de Rocket y consulta los costos por recurso de una Suscripci\u00f3n y Periodo determinado Ej: 2022-02 , 2022-03 . Finalmente retorna los costos en formato JSON . Nota: Los costos solo se pueden consultar con per\u00edodos pasados, y no hay forma de consultar en otros periodos de tiempo. Azure Cost API : Este componente se comunica con el API de Azure Cost Management y consulta los costos por recurso en un intervalo de tiempo. Por ejemplo: desde 2022-07-1 hasta 2022-07-15 . Finalmente se retornan los costos en formato JSON . Notas: En Azure Cost Management se pueden consultar los costos en cualquier intervalo de tiempo en el pasado, es decir puedo hacer consultas en intervalos como: [ 2022-07-1 , 2022-07-15 ]. Tambi\u00e9n es posible consultar los costos del d\u00eda inmediatamente anterior. Por el momento solo se generan costos mensuales pero m\u00e1s adelante se generar\u00e1n costos cada semana(Solo de Azure). Cost Processor : Hay un Cost Processor por cada provider de costos, y tiene el objetivo de procesar los costos obtenidos del provider guardarlos en la base de datos. Todos los Cost Processor guardan la informaci\u00f3n en la misma tabla de la db con el mismo formato. A modo de resumen el procesamiento es encontrar el recurso asociado y el costo total para cada recurso que aparezca en la respuesta del provider. Ejemplo de Tabla de costos : Provider Resource Id Cost Month Year Week Azure ObjectId(\u201c62cc9c1218d23b62caf89d66\u201d) 1222.12121 04 2022 1 Rocket ObjectId(\u201c62cc9c1218d23b62caf89d66\u201d) 1423.232 04 2022 2 Rocket ObjectId(\u201c62cc9c1218d23b62caf89d66\u201d) 1011.2121 04 2022 2 Azure Cost Processor : Para procesar los costos de Azure Cost Management se debe hacer una agrupaci\u00f3n ya que los costos vienen segmentados, para calcular el costo de un solo recurso hay que agrupar al menos 6 registros sumando el costos. Para poder determinar qu\u00e9 registros son del mismo recurso se hace uso de expresiones regulares sobre el Resource Id de cada registro en la respuesta de azure. Ejemplo de los costos segregados para un mismo recurso [ [ 12.8898304 , \"extensionappservice\" , \"microsoft.compute/virtualmachines\" , \"us central\" , \"subscriptions/b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa/resourcegroups/rgvmtemprdp/providers/microsoft.compute/virtualmachines/alidostech-4\" , \"Bandwidth\" , \"USD\" ], [ 0.016945401057600975 , \"extensionappservice\" , \"microsoft.web/disks\" , \"us central\" , \"/subscriptions/b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa/resourcegroups/rgvmtemprdp/providers/microsoft.compute/disks/alidostech-4_disk1_63e1110a0cbf410aaa7e374487b1e5d5\" , \"Disks\" , \"USD\" ], [ 0.019006368 , \"extensionappservice\" , \"microsoft.web/disks\" , \"us central\" , \"/subscriptions/b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa/resourcegroups/rgvmtemprdp/providers/microsoft.compute/disks/alidostech-4_disk2_539ak292a91a374487b1d3992a\" , \"Disks\" , \"USD\" ], [ 0.01767444 , \"extensionappservice\" , \"microsoft.compute/virtualmachines\" , \"us central\" , \"/subscriptions/b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa/resourcegroups/rgvmtemprdp/providers/microsoft.compute/virtualmachines/alidostech-4/extensions/dependencyagentwindows\" , \"Bandwidth\" , \"USD\" ], [ 0.017733688 , \"extensionappservice\" , \"microsoft.web/disks\" , \"us central\" , \"/subscriptions/b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa/resourcegroups/rgvmtemprdp/providers/microsoft.compute/disks/alidostech-4_disk3_102kss229ja2ii2299s92as\" , \"Disks\" , \"USD\" ] ] Rocket Cost Processor : Procesar los costos de Rocket no es una tarea sencilla ya que rocket nos da los siguientes datos para identificar cada recurso: Resource Name Resource Group: extensionappservice , rgsearch , etc. Subscription: Qa , Prod , etc. Service Name: Storage , Virtual Machine , App Service Plan , etc. Como se puede ver no nos da el Resource Type , y sumado a eso hay un problema de ambig\u00fcedad en Service Name causado por azure, por ejemplo varios registros de la respuesta de rocket pueden tener diferentes Service Names y pertenecer al mismo recurso y no hay una forma de determinar cu\u00e1l es el Service Name real para buscar el recurso en la db, pareciera que este se asigna casi de forma aleatoria. Por esta raz\u00f3n se utiliza la respuesta de Azure Cost Management del mismo mes, ya que aqu\u00ed pasa lo mismo hay varios registros con diferentes Service Name para un mismo recurso con la diferencia de que esta le Resource Id , as\u00ed que se puede buscar el registro equivalente y extraer el Resource Id, para luego agrupar los costos por Resource ID . Ejemplo de registros de costos asociados al mismo recurso [ { \"resource\" : \"aks-agentpool-33087089-vmss\" , \"resource_id\" : \"aks-agentpool-33087089-vmss\" , \"resource_category\" : \"Virtual Machines\" , \"parent_id\" : \"mc_rgqaecuaks_qaecuakscluster_eastus2\" , \"total\" : 1164.6898642428978 , \"last_total\" : 1187.6497824401101 , \"forecast\" : 1164.6898642428978 , \"forecast_percentage\" : 0 , \"invoice_type\" : \"1\" , \"estimate\" : 0 , \"usage_issue_date\" : \"0001-01-01T00:00:00Z\" }, { \"resource\" : \"aks-agentpool-33087089-vmss\" , \"resource_id\" : \"aks-agentpool-33087089-vmss\" , \"resource_category\" : \"Bandwidth\" , \"parent_id\" : \"mc_rgqaecuaks_qaecuakscluster_eastus2\" , \"total\" : 19.333820100715656 , \"last_total\" : 16.988908387279764 , \"forecast\" : 19.333820100715656 , \"forecast_percentage\" : 0 , \"invoice_type\" : \"1\" , \"estimate\" : 0 , \"usage_issue_date\" : \"0001-01-01T00:00:00Z\" } ] Siigo Costs API \u00b6 En este momento solo hay un endpoint, este genera un reporte, en formato excel , hay que especificar la siguiente informaci\u00f3n: Periods: Son los periodos que se quieren ver en el reporte, ej: 2022-1 , 2022-2 , 2022-3 . Subscriptions: Son las suscripciones a las cuales se quiere generar costos ej : b3fd9f1c-0ed5-4f6e-9a93-75ae90718vfa , se generar\u00e1 una hoja de excel para cada sub. Providers: Se puede elegir si bajar los costos que se obtuvieron de azure o rocket , a\u00fan no se ha creado una funcionalidad para bajar de ambos al tiempo. Al final el reporte generar\u00e1 dos hojas de c\u00e1lculo por sub, uno con los costos por recurso, otro por grupo de recursos. Ejemplo de una petici\u00f3n con curl. \u00b6 curl -X 'POST' \\ 'https://apisiigoreports.azurewebsites.net/docs#/default/generate_costs_report_excel_report_post/excel_report' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"periods\": [ \"1-2022\",\"2-2022\",\"3-2022\",\"4-2022\",\"5-2022\" ], \"subscriptions\": [ \"b3fd****-****-****-****-*****8vfa\", \"7fef****-****-****-****-**089f788\" ], \"providers\": [ \"rocket\",\"azure\" ], \"minimal_cost\": -1 }' Uso del API desde la Documentacion \u00b6 La documentacion del API esta en el endpoint /docs del api, desde ahi se puede llamar el api de una forma mas grafica.","title":"Siigo Costs"},{"location":"siigo%20billing/#siigo-costs","text":"La aplicaci\u00f3n tiene el objetivo de procesar y guardar los costos de infraestructura de Siigo SAS , de los diferentes clouds azure , aws , oracle , por medio de diferentes providers como rocket , azure costs management , aws cost , para poder generar m\u00e9tricas, indicadores, reportes, donde se puedan ejecutar an\u00e1lisis y de forma oportuna poder responder preguntas como: Donde y porque se incrementaron los costos este mes. Determinar que parte del incremento de costos es de nueva infra y de infra existente. Es el d\u00eda 7, 15, etc. del mes y quiero saber cuales son los recursos que incrementaron sus costos lo que lleva del mes. Cu\u00e1l ser\u00e1 el costo de x recursos el mes que viene(Predicciones avanzadas). Cu\u00e1l es el costo exacto de la nueva infraestructura desplegada este mes. Llevar un hist\u00f3rico de los costos tambi\u00e9n es algo muy valioso, ya que con esto podremos ver como cambian los costos de un recurso, grupo de recursos a lo largo del tiempo. Cu\u00e1l es el descuento exacto que aplic\u00f3 Rocket para cada tipo de recurso el mes que pas\u00f3.","title":"Siigo Costs"},{"location":"siigo%20billing/#diagrama-de-arquitectura","text":"La aplicaci\u00f3n se compone de 3 componentes principales: Siigo Resources Management : Tiene como objetivo bajar los recursos que est\u00e1n desplegados en los diferentes clouds(en este momento solo azure), ya que en los informes de costos se quiere obtener la informaci\u00f3n de cada recurso(tipo, regi\u00f3n, sku, etc.) como tambi\u00e9n los tags(owner, tribu, product, solution). Siigo Cost Processor : Tiene como objetivo bajar los costos de infraestructura de los recursos que est\u00e1n desplegados en los diferentes clouds, por medio de diferentes providers, como Rocket , Azure Cost Management , AWS Costs , para luego procesarlos, agruparlos y relacionarlos a los recursos que est\u00e1n guardados en la base de datos. El objetivo es bajar los costos en intervalos establecidos teniendo en cuenta las restricciones de cada proveedor, por ejemplo: cada mes, cada semana, cada 15 d\u00edas. Siigo Cost API : Es un API enfocada 100% a consultas sobre la info en la db, tiene como objetivo generar diferentes informes de costos, por suscripci\u00f3n, grupo de recursos, recursos, providers, etc, como tambi\u00e9n de recursos, en excel o json . POWER BI : Esta tecnolog\u00eda externa nos ayudar\u00e1 a analizar la informaci\u00f3n de una forma m\u00e1s sencilla, con gr\u00e1ficos, consultas complejas etc, power bi tiene las opciones de obtener datos de un API o una db, en este caso MongoDB.","title":"Diagrama de arquitectura"},{"location":"siigo%20billing/#siigo-resources-management","text":"Por el momento este componente solo baja NUEVOS RECURSOS , m\u00e1s adelante podr\u00eda ser capaz de determinar si un recurso ha sido ELIMINADO o ACTUALIZADO , y reflejarlo en la db. Id Factory : Este componente tiene el objetivo de generar id aleatorios para cada nuevo recurso o grupo de recursos o sub Subscription Fetcher : Este componente tiene el objetivo de consultar las suscripciones de azure a trav\u00e9s del api, procesarlas y retornarlas en formato json. Resource Groups Fetcher : Este componente tiene el objetivo de consultar los recursos de azure a trav\u00e9s del api, procesarlos y retornarlas en formato json. Resources Fetcher : Este componente tiene el objetivo de consultar los recursos de azure a trav\u00e9s del api, procesarlos y retornarlas en formato json. Resources From Costs Fetcher : Este componente existe debido a que no es posible obtener todos los recursos que existieron en un mes con los tres componentes anteriores, ya que el API Azure Resource Management de azure solo retorna los que existen en el momento que se hace la petici\u00f3n. Este componente extrae todos los recursos y grupos de recursos no existentes en la db que vienen en la respuesta de Azure Cost Management los procesa y retorna en en formato json. Cabe aclarar que usando l\u00f3gica difusa se determina cual es el Service Name para cada recurso, ya que en la respuesta de costos para un mismo recurso normalmente vienen registros con diferentes Services Names (Un problema de ambig\u00fcedad). Este proceso se pudo validar en una PoC comparando los resultados con el portal de Azure y se encontr\u00f3 que funciona correctamente para un grupo mediano de datos. Azure Fetcher Main : Este componente tiene el objetivo de llamar a los diferentes fetchers y posteriormente guardar la informaci\u00f3n en la base de datos.","title":"Siigo Resources Management"},{"location":"siigo%20billing/#siigo-costs-processor","text":"Este es un dise\u00f1o simplificado de este m\u00f3dulo, este componente consulta en este momento informaci\u00f3n de costos directamente de Azure y de Rocket , los procesa y guarda en la base de datos. Rocket Cost API : Este componente se comunica con el API de Rocket y consulta los costos por recurso de una Suscripci\u00f3n y Periodo determinado Ej: 2022-02 , 2022-03 . Finalmente retorna los costos en formato JSON . Nota: Los costos solo se pueden consultar con per\u00edodos pasados, y no hay forma de consultar en otros periodos de tiempo. Azure Cost API : Este componente se comunica con el API de Azure Cost Management y consulta los costos por recurso en un intervalo de tiempo. Por ejemplo: desde 2022-07-1 hasta 2022-07-15 . Finalmente se retornan los costos en formato JSON . Notas: En Azure Cost Management se pueden consultar los costos en cualquier intervalo de tiempo en el pasado, es decir puedo hacer consultas en intervalos como: [ 2022-07-1 , 2022-07-15 ]. Tambi\u00e9n es posible consultar los costos del d\u00eda inmediatamente anterior. Por el momento solo se generan costos mensuales pero m\u00e1s adelante se generar\u00e1n costos cada semana(Solo de Azure). Cost Processor : Hay un Cost Processor por cada provider de costos, y tiene el objetivo de procesar los costos obtenidos del provider guardarlos en la base de datos. Todos los Cost Processor guardan la informaci\u00f3n en la misma tabla de la db con el mismo formato. A modo de resumen el procesamiento es encontrar el recurso asociado y el costo total para cada recurso que aparezca en la respuesta del provider. Ejemplo de Tabla de costos : Provider Resource Id Cost Month Year Week Azure ObjectId(\u201c62cc9c1218d23b62caf89d66\u201d) 1222.12121 04 2022 1 Rocket ObjectId(\u201c62cc9c1218d23b62caf89d66\u201d) 1423.232 04 2022 2 Rocket ObjectId(\u201c62cc9c1218d23b62caf89d66\u201d) 1011.2121 04 2022 2 Azure Cost Processor : Para procesar los costos de Azure Cost Management se debe hacer una agrupaci\u00f3n ya que los costos vienen segmentados, para calcular el costo de un solo recurso hay que agrupar al menos 6 registros sumando el costos. Para poder determinar qu\u00e9 registros son del mismo recurso se hace uso de expresiones regulares sobre el Resource Id de cada registro en la respuesta de azure. Ejemplo de los costos segregados para un mismo recurso [ [ 12.8898304 , \"extensionappservice\" , \"microsoft.compute/virtualmachines\" , \"us central\" , \"subscriptions/b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa/resourcegroups/rgvmtemprdp/providers/microsoft.compute/virtualmachines/alidostech-4\" , \"Bandwidth\" , \"USD\" ], [ 0.016945401057600975 , \"extensionappservice\" , \"microsoft.web/disks\" , \"us central\" , \"/subscriptions/b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa/resourcegroups/rgvmtemprdp/providers/microsoft.compute/disks/alidostech-4_disk1_63e1110a0cbf410aaa7e374487b1e5d5\" , \"Disks\" , \"USD\" ], [ 0.019006368 , \"extensionappservice\" , \"microsoft.web/disks\" , \"us central\" , \"/subscriptions/b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa/resourcegroups/rgvmtemprdp/providers/microsoft.compute/disks/alidostech-4_disk2_539ak292a91a374487b1d3992a\" , \"Disks\" , \"USD\" ], [ 0.01767444 , \"extensionappservice\" , \"microsoft.compute/virtualmachines\" , \"us central\" , \"/subscriptions/b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa/resourcegroups/rgvmtemprdp/providers/microsoft.compute/virtualmachines/alidostech-4/extensions/dependencyagentwindows\" , \"Bandwidth\" , \"USD\" ], [ 0.017733688 , \"extensionappservice\" , \"microsoft.web/disks\" , \"us central\" , \"/subscriptions/b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa/resourcegroups/rgvmtemprdp/providers/microsoft.compute/disks/alidostech-4_disk3_102kss229ja2ii2299s92as\" , \"Disks\" , \"USD\" ] ] Rocket Cost Processor : Procesar los costos de Rocket no es una tarea sencilla ya que rocket nos da los siguientes datos para identificar cada recurso: Resource Name Resource Group: extensionappservice , rgsearch , etc. Subscription: Qa , Prod , etc. Service Name: Storage , Virtual Machine , App Service Plan , etc. Como se puede ver no nos da el Resource Type , y sumado a eso hay un problema de ambig\u00fcedad en Service Name causado por azure, por ejemplo varios registros de la respuesta de rocket pueden tener diferentes Service Names y pertenecer al mismo recurso y no hay una forma de determinar cu\u00e1l es el Service Name real para buscar el recurso en la db, pareciera que este se asigna casi de forma aleatoria. Por esta raz\u00f3n se utiliza la respuesta de Azure Cost Management del mismo mes, ya que aqu\u00ed pasa lo mismo hay varios registros con diferentes Service Name para un mismo recurso con la diferencia de que esta le Resource Id , as\u00ed que se puede buscar el registro equivalente y extraer el Resource Id, para luego agrupar los costos por Resource ID . Ejemplo de registros de costos asociados al mismo recurso [ { \"resource\" : \"aks-agentpool-33087089-vmss\" , \"resource_id\" : \"aks-agentpool-33087089-vmss\" , \"resource_category\" : \"Virtual Machines\" , \"parent_id\" : \"mc_rgqaecuaks_qaecuakscluster_eastus2\" , \"total\" : 1164.6898642428978 , \"last_total\" : 1187.6497824401101 , \"forecast\" : 1164.6898642428978 , \"forecast_percentage\" : 0 , \"invoice_type\" : \"1\" , \"estimate\" : 0 , \"usage_issue_date\" : \"0001-01-01T00:00:00Z\" }, { \"resource\" : \"aks-agentpool-33087089-vmss\" , \"resource_id\" : \"aks-agentpool-33087089-vmss\" , \"resource_category\" : \"Bandwidth\" , \"parent_id\" : \"mc_rgqaecuaks_qaecuakscluster_eastus2\" , \"total\" : 19.333820100715656 , \"last_total\" : 16.988908387279764 , \"forecast\" : 19.333820100715656 , \"forecast_percentage\" : 0 , \"invoice_type\" : \"1\" , \"estimate\" : 0 , \"usage_issue_date\" : \"0001-01-01T00:00:00Z\" } ]","title":"Siigo Costs Processor"},{"location":"siigo%20billing/#siigo-costs-api","text":"En este momento solo hay un endpoint, este genera un reporte, en formato excel , hay que especificar la siguiente informaci\u00f3n: Periods: Son los periodos que se quieren ver en el reporte, ej: 2022-1 , 2022-2 , 2022-3 . Subscriptions: Son las suscripciones a las cuales se quiere generar costos ej : b3fd9f1c-0ed5-4f6e-9a93-75ae90718vfa , se generar\u00e1 una hoja de excel para cada sub. Providers: Se puede elegir si bajar los costos que se obtuvieron de azure o rocket , a\u00fan no se ha creado una funcionalidad para bajar de ambos al tiempo. Al final el reporte generar\u00e1 dos hojas de c\u00e1lculo por sub, uno con los costos por recurso, otro por grupo de recursos.","title":"Siigo Costs API"},{"location":"siigo%20billing/#ejemplo-de-una-peticion-con-curl","text":"curl -X 'POST' \\ 'https://apisiigoreports.azurewebsites.net/docs#/default/generate_costs_report_excel_report_post/excel_report' \\ -H 'accept: application/json' \\ -H 'Content-Type: application/json' \\ -d '{ \"periods\": [ \"1-2022\",\"2-2022\",\"3-2022\",\"4-2022\",\"5-2022\" ], \"subscriptions\": [ \"b3fd****-****-****-****-*****8vfa\", \"7fef****-****-****-****-**089f788\" ], \"providers\": [ \"rocket\",\"azure\" ], \"minimal_cost\": -1 }'","title":"Ejemplo de una petici\u00f3n con curl."},{"location":"siigo%20billing/#uso-del-api-desde-la-documentacion","text":"La documentacion del API esta en el endpoint /docs del api, desde ahi se puede llamar el api de una forma mas grafica.","title":"Uso del API desde la Documentacion"},{"location":"labs_kubernetes/","text":"Kubernetes \u00b6 Mis anotaciones de kubernetes Preparacion de laboratorio local en linux \u00b6 Herramientas necesarias: Kubectl # download curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" # check checksum curl -LO \"https://dl.k8s.io/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl.sha256\" echo \" $( cat kubectl.sha256 ) kubectl\" | sha256sum --check # install sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl # check installation kubectl version --client Minikube curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 Quemu Kvm sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager sudo systemctl is-active libvirtd sudo usermod -aG libvirt $USER sudo usermod -aG kvm $USER Minilab Levantar NGINX en un pod en MINIKUBE \u00b6 En este laboratorio se levantara un pod con un servicio de nginx y se expondra el puerto a la maquina local. # Create a cluster minikube start --driver = kvm2 # start the pod running nginx kubectl create deployment --image = nginx nginx-app # [OPTIONAL] add env to nginx-app kubectl set env deployment/nginx-app DOMAIN = cluster # expose a port through with a service kubectl expose deployment nginx-app --port = 80 --name = nginx-http # [TEST] access to service curl 127 .0.0.1:8010 Minilab Levantar imagen local de Docker en Kubernetes \u00b6 En este laboratorio se creara una imagen docker de prueba y se desplegara en un pod . Crear Dockerfile que imprime en pantalla el mensaje Hello World! . FROM busybox CMD [ \"echo\" , \"Hello World!\" ] Crear la imagen Docker. $ docker build . -t santos/hello-world # Sending build context to Docker daemon 3.072kB # Step 1/2 : FROM busybox # ---> 62aedd01bd85 # Step 2/2 : CMD [ \"echo\", \"Hello World!\"] # ---> Using cache # ---> 79b4e6c1bd0d # Successfully built 79b4e6c1bd0d # Successfully tagged santos/hello-world:latest Levantar un contenedor de prueba. $ docker run santos/hello-world # Hello World! Crear el archivo de configuracion helloworld.yml para crear un job de kubernetes que corra el container Docker. apiVersion : batch/v1 kind : Job metadata : name : hello-world spec : template : metadata : name : hello-world-pod spec : containers : - name : hello-world image : santos/hello-world imagePullPolicy : Never restartPolicy : Never restartPolicy es Never debido que el container solo va a ejecutar el mensaje Hello World! y terminar, si se deja en Always se va a ejecutar infinitas veces. imagePullPolicy es Never debido a que se quiere que busque la imagen localmente solamente, si no la encuentra no intentara bajarla del container registry. Crear el job de kubernetes con este archivo de configuracion y kubectl . $ kubectl create -f helloworld.yml # job.batch/hello-world created Revisar si se ejecuto correctamente. $ kubectl get pods # NAME READY STATUS RESTARTS AGE # hello-world-r4g9g 0/1 ErrImageNeverPull 0 6s El error ErrImageNeverPull significa que el nodo minikube usa su propio repositorio de Docker que no est\u00e1 conectado al registro de Docker en la m\u00e1quina local, por lo que, sin extraer, no sabe de d\u00f3nde obtener la imagen. Para solucionar esto, utilizo el comando minikube docker-env que genera las variables de entorno necesarias para apuntar el demonio local de Docker al registro interno de Docker de minikube: $ minikube docker-env # export DOCKER_TLS_VERIFY=\u201d1\" # export DOCKER_HOST=\u201dtcp://172.17.0.2:2376\" # export DOCKER_CERT_PATH=\u201d/home/user/.minikube/certs\u201d # export MINIKUBE_ACTIVE_DOCKERD=\u201dminikube\u201d# To point your shell to minikube\u2019s docker-daemon, run: ## eval $(minikube -p minikube docker-env) Aplicar las variables de minikube $ eval $( minikube -p minikube docker-env ) Construyo la imagen de docker nuevamente, esta vez en el registro interno de minukube . $ docker build . -t santos/hello-world Recrear el job otra vez. $ kubectl delete -f helloworld.yml $ kubectl create -f helloworld.yml Revisar si se ejecuto correctamente. $ kubectl get pods # NAME READY STATUS RESTARTS AGE # hello-world-r4g9g 0/1 Completed 0 6s Revisar los logs del pod. $ kubectl logs hello-world-f5hzz # Hello World!","title":"Kubernetes"},{"location":"labs_kubernetes/#kubernetes","text":"Mis anotaciones de kubernetes","title":"Kubernetes"},{"location":"labs_kubernetes/#preparacion-de-laboratorio-local-en-linux","text":"Herramientas necesarias: Kubectl # download curl -LO \"https://dl.k8s.io/release/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl\" # check checksum curl -LO \"https://dl.k8s.io/ $( curl -L -s https://dl.k8s.io/release/stable.txt ) /bin/linux/amd64/kubectl.sha256\" echo \" $( cat kubectl.sha256 ) kubectl\" | sha256sum --check # install sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl # check installation kubectl version --client Minikube curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 Quemu Kvm sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager sudo systemctl is-active libvirtd sudo usermod -aG libvirt $USER sudo usermod -aG kvm $USER","title":"Preparacion de laboratorio local en linux"},{"location":"labs_kubernetes/#minilab-levantar-nginx-en-un-pod-en-minikube","text":"En este laboratorio se levantara un pod con un servicio de nginx y se expondra el puerto a la maquina local. # Create a cluster minikube start --driver = kvm2 # start the pod running nginx kubectl create deployment --image = nginx nginx-app # [OPTIONAL] add env to nginx-app kubectl set env deployment/nginx-app DOMAIN = cluster # expose a port through with a service kubectl expose deployment nginx-app --port = 80 --name = nginx-http # [TEST] access to service curl 127 .0.0.1:8010","title":"Minilab Levantar NGINX en un pod en MINIKUBE"},{"location":"labs_kubernetes/#minilab-levantar-imagen-local-de-docker-en-kubernetes","text":"En este laboratorio se creara una imagen docker de prueba y se desplegara en un pod . Crear Dockerfile que imprime en pantalla el mensaje Hello World! . FROM busybox CMD [ \"echo\" , \"Hello World!\" ] Crear la imagen Docker. $ docker build . -t santos/hello-world # Sending build context to Docker daemon 3.072kB # Step 1/2 : FROM busybox # ---> 62aedd01bd85 # Step 2/2 : CMD [ \"echo\", \"Hello World!\"] # ---> Using cache # ---> 79b4e6c1bd0d # Successfully built 79b4e6c1bd0d # Successfully tagged santos/hello-world:latest Levantar un contenedor de prueba. $ docker run santos/hello-world # Hello World! Crear el archivo de configuracion helloworld.yml para crear un job de kubernetes que corra el container Docker. apiVersion : batch/v1 kind : Job metadata : name : hello-world spec : template : metadata : name : hello-world-pod spec : containers : - name : hello-world image : santos/hello-world imagePullPolicy : Never restartPolicy : Never restartPolicy es Never debido que el container solo va a ejecutar el mensaje Hello World! y terminar, si se deja en Always se va a ejecutar infinitas veces. imagePullPolicy es Never debido a que se quiere que busque la imagen localmente solamente, si no la encuentra no intentara bajarla del container registry. Crear el job de kubernetes con este archivo de configuracion y kubectl . $ kubectl create -f helloworld.yml # job.batch/hello-world created Revisar si se ejecuto correctamente. $ kubectl get pods # NAME READY STATUS RESTARTS AGE # hello-world-r4g9g 0/1 ErrImageNeverPull 0 6s El error ErrImageNeverPull significa que el nodo minikube usa su propio repositorio de Docker que no est\u00e1 conectado al registro de Docker en la m\u00e1quina local, por lo que, sin extraer, no sabe de d\u00f3nde obtener la imagen. Para solucionar esto, utilizo el comando minikube docker-env que genera las variables de entorno necesarias para apuntar el demonio local de Docker al registro interno de Docker de minikube: $ minikube docker-env # export DOCKER_TLS_VERIFY=\u201d1\" # export DOCKER_HOST=\u201dtcp://172.17.0.2:2376\" # export DOCKER_CERT_PATH=\u201d/home/user/.minikube/certs\u201d # export MINIKUBE_ACTIVE_DOCKERD=\u201dminikube\u201d# To point your shell to minikube\u2019s docker-daemon, run: ## eval $(minikube -p minikube docker-env) Aplicar las variables de minikube $ eval $( minikube -p minikube docker-env ) Construyo la imagen de docker nuevamente, esta vez en el registro interno de minukube . $ docker build . -t santos/hello-world Recrear el job otra vez. $ kubectl delete -f helloworld.yml $ kubectl create -f helloworld.yml Revisar si se ejecuto correctamente. $ kubectl get pods # NAME READY STATUS RESTARTS AGE # hello-world-r4g9g 0/1 Completed 0 6s Revisar los logs del pod. $ kubectl logs hello-world-f5hzz # Hello World!","title":"Minilab Levantar  imagen local de Docker en Kubernetes"},{"location":"labs_kubernetes/labs1/","text":"Load Balancer y una API Flask \u00b6 En este laboratorio se desplegara una aplicacion dockerizada hecha en python en kubernetes , agregando un LoadBalancer . Crear el app de python #save as app.py from flask import Flask app = Flask ( __name__ ) @app . route ( \"/\" ) def hello (): return \"hello word from flask!\" if __name__ == \"__main__\" : app . run ( host = '0.0.0.0' , debug = True ) Crear el Dockerfile FROM python:3.7 RUN mkdir /app WORKDIR /app/ ADD . /app/ RUN pip install flask CMD [ \"python\" , \"/app/app.py\" ] Construir la imagen, pero antes apuntar al registry de minukube . $ eval $( minikube -p minikube docker-env ) $ docker build . -t santos/flask-hello-world Crear archivo de configuracion deployment.yml apiVersion : v1 kind : Service metadata : name : flask-test-service spec : selector : app : flask-test-app # en nombre de la aplicacion que va a gestionar ports : - protocol : \"TCP\" port : 6000 targetPort : 5000 # El puerto donde se expone la aplicacion Flask type : LoadBalancer --- apiVersion : apps/v1 kind : Deployment metadata : name : flask-test-app spec : selector : matchLabels : app : flask-test-app replicas : 5 # template : metadata : labels : app : flask-test-app spec : containers : - name : flask-test-app image : santos/flask-hello-world imagePullPolicy : Never ports : - containerPort : 5000 Aplicar el archivo de configuracion con kubectl . $ kubectl create -f deployment.yml # service/flask-test-service created # deployment.apps/flask-test-app created Revisar si se ejecuto correctamente. $ kubectl get pods # NAME READY STATUS RESTARTS AGE # flask-test-app-c6b649857-2tgs5 1/1 Running 0 14s # flask-test-app-c6b649857-g7f28 1/1 Running 0 14s # flask-test-app-c6b649857-lh4nb 1/1 Running 0 14s # flask-test-app-c6b649857-rsdjr 1/1 Running 0 14s # flask-test-app-c6b649857-vmsj5 1/1 Running 0 14s $ kubectl get services # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # flask-test-service LoadBalancer 10.102.69.135 <pending> 6000:31902/TCP 81s Obtener la url de acceso al LoadBalancer. Como se esta usando minkube se utiliza el siguiente comando. $ minikube service example-service --url # http://192.168.39.232:31902 Si no se esta usando minikube se utiliza kubectl describe $ kubectl describe services flask-test-service # Name: flask-test-service # Namespace: default # Labels: <none> # Annotations: <none> # Selector: app=flask-test-app # Type: LoadBalancer # IP Family Policy: SingleStack # IP Families: IPv4 # IP: 10.102.69.135 # IPs: 10.102.69.135 # LoadBalancer Ingress: 192.168.39.232 # Port: <unset> 6000/TCP # TargetPort: 5000/TCP # NodePort: <unset> 31902/TCP # Endpoints: 172.17.0.12:5000,172.17.0.3:5000,172.17.0.6:5000 + 2 more... # Session Affinity: None # External Traffic Policy: Cluster # Events: <none> En la salida del comando se puede ver informacion del LoadBalancer , el primero que necesitamos es LoadBalancer Ingress . Hacer un get al API: $ curl http://192.168.39.232:31902 # hello word from flask!","title":"Load Balancer y una API Flask"},{"location":"labs_kubernetes/labs1/#load-balancer-y-una-api-flask","text":"En este laboratorio se desplegara una aplicacion dockerizada hecha en python en kubernetes , agregando un LoadBalancer . Crear el app de python #save as app.py from flask import Flask app = Flask ( __name__ ) @app . route ( \"/\" ) def hello (): return \"hello word from flask!\" if __name__ == \"__main__\" : app . run ( host = '0.0.0.0' , debug = True ) Crear el Dockerfile FROM python:3.7 RUN mkdir /app WORKDIR /app/ ADD . /app/ RUN pip install flask CMD [ \"python\" , \"/app/app.py\" ] Construir la imagen, pero antes apuntar al registry de minukube . $ eval $( minikube -p minikube docker-env ) $ docker build . -t santos/flask-hello-world Crear archivo de configuracion deployment.yml apiVersion : v1 kind : Service metadata : name : flask-test-service spec : selector : app : flask-test-app # en nombre de la aplicacion que va a gestionar ports : - protocol : \"TCP\" port : 6000 targetPort : 5000 # El puerto donde se expone la aplicacion Flask type : LoadBalancer --- apiVersion : apps/v1 kind : Deployment metadata : name : flask-test-app spec : selector : matchLabels : app : flask-test-app replicas : 5 # template : metadata : labels : app : flask-test-app spec : containers : - name : flask-test-app image : santos/flask-hello-world imagePullPolicy : Never ports : - containerPort : 5000 Aplicar el archivo de configuracion con kubectl . $ kubectl create -f deployment.yml # service/flask-test-service created # deployment.apps/flask-test-app created Revisar si se ejecuto correctamente. $ kubectl get pods # NAME READY STATUS RESTARTS AGE # flask-test-app-c6b649857-2tgs5 1/1 Running 0 14s # flask-test-app-c6b649857-g7f28 1/1 Running 0 14s # flask-test-app-c6b649857-lh4nb 1/1 Running 0 14s # flask-test-app-c6b649857-rsdjr 1/1 Running 0 14s # flask-test-app-c6b649857-vmsj5 1/1 Running 0 14s $ kubectl get services # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # flask-test-service LoadBalancer 10.102.69.135 <pending> 6000:31902/TCP 81s Obtener la url de acceso al LoadBalancer. Como se esta usando minkube se utiliza el siguiente comando. $ minikube service example-service --url # http://192.168.39.232:31902 Si no se esta usando minikube se utiliza kubectl describe $ kubectl describe services flask-test-service # Name: flask-test-service # Namespace: default # Labels: <none> # Annotations: <none> # Selector: app=flask-test-app # Type: LoadBalancer # IP Family Policy: SingleStack # IP Families: IPv4 # IP: 10.102.69.135 # IPs: 10.102.69.135 # LoadBalancer Ingress: 192.168.39.232 # Port: <unset> 6000/TCP # TargetPort: 5000/TCP # NodePort: <unset> 31902/TCP # Endpoints: 172.17.0.12:5000,172.17.0.3:5000,172.17.0.6:5000 + 2 more... # Session Affinity: None # External Traffic Policy: Cluster # Events: <none> En la salida del comando se puede ver informacion del LoadBalancer , el primero que necesitamos es LoadBalancer Ingress . Hacer un get al API: $ curl http://192.168.39.232:31902 # hello word from flask!","title":"Load Balancer y una API Flask"},{"location":"labs_kubernetes/labs2/","text":"Compartir archivos entre dos containers en un Pod \u00b6 En este laboratorio se comunicaran dos containers que viven en un mismo pod usando volumenes . Crear el el pod con un volumen , y dos containers # save as ./two-containers-and-volume.yaml apiVersion : v1 kind : Pod metadata : name : two-containers spec : restartPolicy : Never volumes : - name : shared-data emptyDir : {} containers : - name : nginx-container image : nginx volumeMounts : - name : shared-data mountPath : /usr/share/nginx/html - name : debian-container image : debian volumeMounts : - name : shared-data mountPath : /pod-data command : [ \"/bin/sh\" ] args : [ \"-c\" , \"echo Hello from the debian container > /pod-data/index.html\" ] Como se puede ver se crea un Pod llamado two-containers y dentro de la especificacion se crea un volumen llamado shared-data , luego se crea un container llamado nginx-container montando el volumen en /usr/share/nginx/html , y por ultimo se crea el container llamado debian-container , montando el volumen en /pod-data . El segundo container ejecutara el siguiente comando y terminara su ejecucion. $ echo Hello from the debian container > /pod-data/index.html El segundo container modificara el archivo index.html de la carpeta principal del servidor nginx del segundo container, ya que este esta en el volumen compartido. Crear el pod usando kubectl $ kubectl apply -f ./two-containers-and-volume.yaml # pod/two-containers created Para ver la informacion detallada del pod $ kubectl get pod two-containers --output = yaml output kubectl get pod two-containers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 apiVersion : v1 kind : Pod metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"two-containers\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"nginx\",\"name\":\"nginx-container\",\"volumeMounts\":[{\"mountPath\":\"/usr/share/nginx/html\",\"name\":\"shared-data\"}]},{\"args\":[\"-c\",\"echo Hello from the debian container \\u003e /pod-data/index.html\"],\"command\":[\"/bin/sh\"],\"image\":\"debian\",\"name\":\"debian-container\",\"volumeMounts\":[{\"mountPath\":\"/pod-data\",\"name\":\"shared-data\"}]}],\"restartPolicy\":\"Never\",\"volumes\":[{\"emptyDir\":{},\"name\":\"shared-data\"}]}} creationTimestamp : \"2022-06-16T14:26:55Z\" name : two-containers namespace : default resourceVersion : \"35653\" uid : a2f25f74-e860-465d-b0e9-1fbdfa9d8a1a spec : containers : - image : nginx imagePullPolicy : Always name : nginx-container resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /usr/share/nginx/html name : shared-data - mountPath : /var/run/secrets/kubernetes.io/serviceaccount name : kube-api-access-mvrgj readOnly : true - args : - -c - echo Hello from the debian container > /pod-data/index.html command : - /bin/sh image : debian imagePullPolicy : Always name : debian-container resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /pod-data name : shared-data - mountPath : /var/run/secrets/kubernetes.io/serviceaccount name : kube-api-access-mvrgj readOnly : true dnsPolicy : ClusterFirst enableServiceLinks : true nodeName : minikube preemptionPolicy : PreemptLowerPriority priority : 0 restartPolicy : Never schedulerName : default-scheduler securityContext : {} serviceAccount : default serviceAccountName : default terminationGracePeriodSeconds : 30 tolerations : - effect : NoExecute key : node.kubernetes.io/not-ready operator : Exists tolerationSeconds : 300 - effect : NoExecute key : node.kubernetes.io/unreachable operator : Exists tolerationSeconds : 300 volumes : - emptyDir : {} name : shared-data - name : kube-api-access-mvrgj projected : defaultMode : 420 sources : - serviceAccountToken : expirationSeconds : 3607 path : token - configMap : items : - key : ca.crt path : ca.crt name : kube-root-ca.crt - downwardAPI : items : - fieldRef : apiVersion : v1 fieldPath : metadata.namespace path : namespace status : conditions : - lastProbeTime : null lastTransitionTime : \"2022-06-16T14:26:55Z\" status : \"True\" type : Initialized - lastProbeTime : null lastTransitionTime : \"2022-06-16T14:26:55Z\" message : 'containers with unready status: [debian-container]' reason : ContainersNotReady status : \"False\" type : Ready - lastProbeTime : null lastTransitionTime : \"2022-06-16T14:26:55Z\" message : 'containers with unready status: [debian-container]' reason : ContainersNotReady status : \"False\" type : ContainersReady - lastProbeTime : null lastTransitionTime : \"2022-06-16T14:26:55Z\" status : \"True\" type : PodScheduled containerStatuses : - containerID : docker://fc3b98f854b9bb2a26c6abbe9879bffa4ae93ee16907aa725a99a5f90699a3c5 image : debian:latest imageID : docker-pullable://debian@sha256:3f1d6c17773a45c97bd8f158d665c9709d7b29ed7917ac934086ad96f92e4510 lastState : {} name : debian-container ready : false # (1) restartCount : 0 started : false state : terminated : containerID : docker://fc3b98f854b9bb2a26c6abbe9879bffa4ae93ee16907aa725a99a5f90699a3c5 exitCode : 0 finishedAt : \"2022-06-16T14:27:00Z\" reason : Completed # (2) startedAt : \"2022-06-16T14:27:00Z\" - containerID : docker://00c92009ff5b762f5a26552f1515addbe118403ab4efbb1e2d1244c698f0ccb2 image : nginx:latest imageID : docker-pullable://nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514 lastState : {} name : nginx-container ready : true restartCount : 0 started : true state : running : startedAt : \"2022-06-16T14:26:57Z\" hostIP : 192.168.39.232 phase : Running # (3) podIP : 172.17.0.16 podIPs : - ip : 172.17.0.16 qosClass : BestEffort startTime : \"2022-06-16T14:26:55Z\" Es false debido a que el contenedor solo ejecuta un echo y termina su ejecucion. El container termino su ejecucion con exito . El container nginx esta corriendo porque es un servidor . Se puede ver que container debian-container termino su ejecucion y nginx-container sigue corriendo. Entrar al contenedor nginx-container $ kubectl exec -it two-containers -c nginx-container -- /bin/bash Revisar el archivo index.html de la carpeta compartida root@two-containers:/# cat /usr/share/nginx/html/index.html # Hello from the debian container Revisar la respuesta del servidor de nginx root@two-containers:/# curl localhost # Hello from the debian container { \"data\" :{ \"attributes\" :{ \"resource_names_by_hash\" :{ \"7d74f4592a9fd6ff\" : \"POST businessloki.loki.svc.cluster.local:3100/loki/api/v1/push\" , \"5fc09ce97131f406\" : \"POST dc.services.visualstudio.com/v2/track\" , \"424ed4ae196ebbaf\" : \"POST rt.services.visualstudio.com/QuickPulseService.svc/ping\" }}, \"type\" : \"resource_names_by_hash\" }}","title":"Compartir archivos entre  dos containers en un Pod"},{"location":"labs_kubernetes/labs2/#compartir-archivos-entre-dos-containers-en-un-pod","text":"En este laboratorio se comunicaran dos containers que viven en un mismo pod usando volumenes . Crear el el pod con un volumen , y dos containers # save as ./two-containers-and-volume.yaml apiVersion : v1 kind : Pod metadata : name : two-containers spec : restartPolicy : Never volumes : - name : shared-data emptyDir : {} containers : - name : nginx-container image : nginx volumeMounts : - name : shared-data mountPath : /usr/share/nginx/html - name : debian-container image : debian volumeMounts : - name : shared-data mountPath : /pod-data command : [ \"/bin/sh\" ] args : [ \"-c\" , \"echo Hello from the debian container > /pod-data/index.html\" ] Como se puede ver se crea un Pod llamado two-containers y dentro de la especificacion se crea un volumen llamado shared-data , luego se crea un container llamado nginx-container montando el volumen en /usr/share/nginx/html , y por ultimo se crea el container llamado debian-container , montando el volumen en /pod-data . El segundo container ejecutara el siguiente comando y terminara su ejecucion. $ echo Hello from the debian container > /pod-data/index.html El segundo container modificara el archivo index.html de la carpeta principal del servidor nginx del segundo container, ya que este esta en el volumen compartido. Crear el pod usando kubectl $ kubectl apply -f ./two-containers-and-volume.yaml # pod/two-containers created Para ver la informacion detallada del pod $ kubectl get pod two-containers --output = yaml output kubectl get pod two-containers 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 apiVersion : v1 kind : Pod metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"two-containers\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"nginx\",\"name\":\"nginx-container\",\"volumeMounts\":[{\"mountPath\":\"/usr/share/nginx/html\",\"name\":\"shared-data\"}]},{\"args\":[\"-c\",\"echo Hello from the debian container \\u003e /pod-data/index.html\"],\"command\":[\"/bin/sh\"],\"image\":\"debian\",\"name\":\"debian-container\",\"volumeMounts\":[{\"mountPath\":\"/pod-data\",\"name\":\"shared-data\"}]}],\"restartPolicy\":\"Never\",\"volumes\":[{\"emptyDir\":{},\"name\":\"shared-data\"}]}} creationTimestamp : \"2022-06-16T14:26:55Z\" name : two-containers namespace : default resourceVersion : \"35653\" uid : a2f25f74-e860-465d-b0e9-1fbdfa9d8a1a spec : containers : - image : nginx imagePullPolicy : Always name : nginx-container resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /usr/share/nginx/html name : shared-data - mountPath : /var/run/secrets/kubernetes.io/serviceaccount name : kube-api-access-mvrgj readOnly : true - args : - -c - echo Hello from the debian container > /pod-data/index.html command : - /bin/sh image : debian imagePullPolicy : Always name : debian-container resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /pod-data name : shared-data - mountPath : /var/run/secrets/kubernetes.io/serviceaccount name : kube-api-access-mvrgj readOnly : true dnsPolicy : ClusterFirst enableServiceLinks : true nodeName : minikube preemptionPolicy : PreemptLowerPriority priority : 0 restartPolicy : Never schedulerName : default-scheduler securityContext : {} serviceAccount : default serviceAccountName : default terminationGracePeriodSeconds : 30 tolerations : - effect : NoExecute key : node.kubernetes.io/not-ready operator : Exists tolerationSeconds : 300 - effect : NoExecute key : node.kubernetes.io/unreachable operator : Exists tolerationSeconds : 300 volumes : - emptyDir : {} name : shared-data - name : kube-api-access-mvrgj projected : defaultMode : 420 sources : - serviceAccountToken : expirationSeconds : 3607 path : token - configMap : items : - key : ca.crt path : ca.crt name : kube-root-ca.crt - downwardAPI : items : - fieldRef : apiVersion : v1 fieldPath : metadata.namespace path : namespace status : conditions : - lastProbeTime : null lastTransitionTime : \"2022-06-16T14:26:55Z\" status : \"True\" type : Initialized - lastProbeTime : null lastTransitionTime : \"2022-06-16T14:26:55Z\" message : 'containers with unready status: [debian-container]' reason : ContainersNotReady status : \"False\" type : Ready - lastProbeTime : null lastTransitionTime : \"2022-06-16T14:26:55Z\" message : 'containers with unready status: [debian-container]' reason : ContainersNotReady status : \"False\" type : ContainersReady - lastProbeTime : null lastTransitionTime : \"2022-06-16T14:26:55Z\" status : \"True\" type : PodScheduled containerStatuses : - containerID : docker://fc3b98f854b9bb2a26c6abbe9879bffa4ae93ee16907aa725a99a5f90699a3c5 image : debian:latest imageID : docker-pullable://debian@sha256:3f1d6c17773a45c97bd8f158d665c9709d7b29ed7917ac934086ad96f92e4510 lastState : {} name : debian-container ready : false # (1) restartCount : 0 started : false state : terminated : containerID : docker://fc3b98f854b9bb2a26c6abbe9879bffa4ae93ee16907aa725a99a5f90699a3c5 exitCode : 0 finishedAt : \"2022-06-16T14:27:00Z\" reason : Completed # (2) startedAt : \"2022-06-16T14:27:00Z\" - containerID : docker://00c92009ff5b762f5a26552f1515addbe118403ab4efbb1e2d1244c698f0ccb2 image : nginx:latest imageID : docker-pullable://nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514 lastState : {} name : nginx-container ready : true restartCount : 0 started : true state : running : startedAt : \"2022-06-16T14:26:57Z\" hostIP : 192.168.39.232 phase : Running # (3) podIP : 172.17.0.16 podIPs : - ip : 172.17.0.16 qosClass : BestEffort startTime : \"2022-06-16T14:26:55Z\" Es false debido a que el contenedor solo ejecuta un echo y termina su ejecucion. El container termino su ejecucion con exito . El container nginx esta corriendo porque es un servidor . Se puede ver que container debian-container termino su ejecucion y nginx-container sigue corriendo. Entrar al contenedor nginx-container $ kubectl exec -it two-containers -c nginx-container -- /bin/bash Revisar el archivo index.html de la carpeta compartida root@two-containers:/# cat /usr/share/nginx/html/index.html # Hello from the debian container Revisar la respuesta del servidor de nginx root@two-containers:/# curl localhost # Hello from the debian container { \"data\" :{ \"attributes\" :{ \"resource_names_by_hash\" :{ \"7d74f4592a9fd6ff\" : \"POST businessloki.loki.svc.cluster.local:3100/loki/api/v1/push\" , \"5fc09ce97131f406\" : \"POST dc.services.visualstudio.com/v2/track\" , \"424ed4ae196ebbaf\" : \"POST rt.services.visualstudio.com/QuickPulseService.svc/ping\" }}, \"type\" : \"resource_names_by_hash\" }}","title":"Compartir archivos entre  dos containers en un Pod"},{"location":"labs_kubernetes/labs3/","text":"Ping desde entre dos containers de un mismo POD \u00b6 En este laboratorio se mostrara que se pueden alcanzar containers en un mismo pod. En este laboratorio se usara la imagen santos/flask-hello-world de laboratorios atras, y tambien usara una configuracion sencilla nginx. Crear el archivo de configuracion de nginx nginx-vol/default.conf el el directorio que vamos a enlazar con un volumen al container. server { listen 80; listen [::]:80; server_name localhost; location /flask_app { proxy_pass http://127.0.0.1:5000/; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Esta pequena configuracion sencilla de ngnx crea un proxy_pass de localhost/flask_app a localhost:5000 , este es el puerto de escucha de la aplicacion de flask que se usara en este laboratorio. Recordar que kubernetes no tiene comunicacion directa con nuestra maquina, tiene comunicacion directa con la maquina virtual que monta minikube, como se quiere montar un directorio de nuestra maquina como un volumen, primero se debe montar en la maquina virtual de minikube. minikube mount nginx-vol/:/mynginx/ # \ud83d\udcc1 Mounting host path myvol/ into VM as /mynginx/ ... # \u25aa Mount type: # \u25aa User ID: docker # \u25aa Group ID: docker # \u25aa Version: 9p2000.L # \u25aa Message Size: 262144 # \u25aa Options: map[] # \u25aa Bind Address: 192.168.39.1:40637 # \ud83d\ude80 Userspace file server: ufs starting # \u2705 Successfully mounted myvol/ to /mynginx/ # \ud83d\udccc NOTE: This process must stay alive for the mount to be accessible ... El volumen estara montado mientras no terminemos este proceso. Ahora existe una carpeta llamada /mynginx en la VM . Yaml de creacion del Pod # save as ./spec.yaml apiVersion : v1 kind : Pod metadata : name : two-containers-ping spec : restartPolicy : Never volumes : - name : shared-data hostPath : path : /mynginx/ type : Directory containers : - name : nginx-container image : nginx volumeMounts : - name : shared-data mountPath : /etc/nginx/conf.d - name : flask-test-app image : santos/flask-hello-world imagePullPolicy : Never # ports: # - containerPort: 5000 Crear el pod usando kubectl $ kubectl apply -f ./spec.yaml # pod/two-containers-ping created Revisar el estado del nuevo pod kubectl get pod two-containers-ping --output = yaml output kubectl get pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 apiVersion : v1 kind : Pod metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"two-containers-ping\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"nginx\",\"name\":\"nginx-container\",\"volumeMounts\":[{\"mountPath\":\"/etc/nginx/conf.d\",\"name\":\"shared-data\"}]},{\"image\":\"santos/flask-hello-world\",\"imagePullPolicy\":\"Never\",\"name\":\"flask-test-app\"}],\"restartPolicy\":\"Never\",\"volumes\":[{\"hostPath\":{\"path\":\"/mynginx/\",\"type\":\"Directory\"},\"name\":\"shared-data\"}]}} creationTimestamp : \"2022-06-17T14:38:29Z\" name : two-containers-ping namespace : default resourceVersion : \"52327\" uid : 6dbb4caa-87b3-498a-8c4a-a7aeb76842af spec : containers : - image : nginx imagePullPolicy : Always name : nginx-container resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /etc/nginx/conf.d name : shared-data - mountPath : /var/run/secrets/kubernetes.io/serviceaccount name : kube-api-access-8vpjq readOnly : true - image : santos/flask-hello-world imagePullPolicy : Never name : flask-test-app resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /var/run/secrets/kubernetes.io/serviceaccount name : kube-api-access-8vpjq readOnly : true dnsPolicy : ClusterFirst enableServiceLinks : true nodeName : minikube preemptionPolicy : PreemptLowerPriority priority : 0 restartPolicy : Never schedulerName : default-scheduler securityContext : {} serviceAccount : default serviceAccountName : default terminationGracePeriodSeconds : 30 tolerations : - effect : NoExecute key : node.kubernetes.io/not-ready operator : Exists tolerationSeconds : 300 - effect : NoExecute key : node.kubernetes.io/unreachable operator : Exists tolerationSeconds : 300 volumes : - hostPath : path : /mynginx/ type : Directory name : shared-data - name : kube-api-access-8vpjq projected : defaultMode : 420 sources : - serviceAccountToken : expirationSeconds : 3607 path : token - configMap : items : - key : ca.crt path : ca.crt name : kube-root-ca.crt - downwardAPI : items : - fieldRef : apiVersion : v1 fieldPath : metadata.namespace path : namespace status : conditions : - lastProbeTime : null lastTransitionTime : \"2022-06-17T14:38:29Z\" status : \"True\" type : Initialized - lastProbeTime : null lastTransitionTime : \"2022-06-17T14:38:31Z\" status : \"True\" type : Ready - lastProbeTime : null lastTransitionTime : \"2022-06-17T14:38:31Z\" status : \"True\" type : ContainersReady - lastProbeTime : null lastTransitionTime : \"2022-06-17T14:38:29Z\" status : \"True\" type : PodScheduled containerStatuses : - containerID : docker://0b53b339b7c06b7475739cbde17795ce24f71bc8d7d2e41da07d220162b6475d image : santos/flask-hello-world:latest imageID : docker://sha256:105bc09b0a7e87f7344040742f1fa011e00e7a36bac1f1cabc8a5303ceb1db30 lastState : {} name : flask-test-app ready : true restartCount : 0 started : true state : running : startedAt : \"2022-06-17T14:38:31Z\" - containerID : docker://8175ac7cd60060159888a723b7a36a93486a38e9b6cd4379d7e86fbb4647375d image : nginx:latest imageID : docker-pullable://nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514 lastState : {} name : nginx-container ready : true restartCount : 0 started : true state : running : startedAt : \"2022-06-17T14:38:31Z\" hostIP : 192.168.39.232 phase : Running podIP : 172.17.0.16 podIPs : - ip : 172.17.0.16 qosClass : BestEffort startTime : \"2022-06-17T14:38:29Z\" Como se puede ver los dos containers estan Ready Entrar al container nginx-container $ kubectl exec -it two-containers-ping -c nginx-container -- /bin/bash # root@two-containers-ping:/ Hacer un get a http://localhost/flask_app/ $root @two-containers-ping: curl http://localhost/flask_app # hello word from flask! Conclusion \u00b6 Containers within a pod share an IP address and port space, and can find each other via localhost s","title":"Ping desde entre dos containers de un mismo POD"},{"location":"labs_kubernetes/labs3/#ping-desde-entre-dos-containers-de-un-mismo-pod","text":"En este laboratorio se mostrara que se pueden alcanzar containers en un mismo pod. En este laboratorio se usara la imagen santos/flask-hello-world de laboratorios atras, y tambien usara una configuracion sencilla nginx. Crear el archivo de configuracion de nginx nginx-vol/default.conf el el directorio que vamos a enlazar con un volumen al container. server { listen 80; listen [::]:80; server_name localhost; location /flask_app { proxy_pass http://127.0.0.1:5000/; } error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } } Esta pequena configuracion sencilla de ngnx crea un proxy_pass de localhost/flask_app a localhost:5000 , este es el puerto de escucha de la aplicacion de flask que se usara en este laboratorio. Recordar que kubernetes no tiene comunicacion directa con nuestra maquina, tiene comunicacion directa con la maquina virtual que monta minikube, como se quiere montar un directorio de nuestra maquina como un volumen, primero se debe montar en la maquina virtual de minikube. minikube mount nginx-vol/:/mynginx/ # \ud83d\udcc1 Mounting host path myvol/ into VM as /mynginx/ ... # \u25aa Mount type: # \u25aa User ID: docker # \u25aa Group ID: docker # \u25aa Version: 9p2000.L # \u25aa Message Size: 262144 # \u25aa Options: map[] # \u25aa Bind Address: 192.168.39.1:40637 # \ud83d\ude80 Userspace file server: ufs starting # \u2705 Successfully mounted myvol/ to /mynginx/ # \ud83d\udccc NOTE: This process must stay alive for the mount to be accessible ... El volumen estara montado mientras no terminemos este proceso. Ahora existe una carpeta llamada /mynginx en la VM . Yaml de creacion del Pod # save as ./spec.yaml apiVersion : v1 kind : Pod metadata : name : two-containers-ping spec : restartPolicy : Never volumes : - name : shared-data hostPath : path : /mynginx/ type : Directory containers : - name : nginx-container image : nginx volumeMounts : - name : shared-data mountPath : /etc/nginx/conf.d - name : flask-test-app image : santos/flask-hello-world imagePullPolicy : Never # ports: # - containerPort: 5000 Crear el pod usando kubectl $ kubectl apply -f ./spec.yaml # pod/two-containers-ping created Revisar el estado del nuevo pod kubectl get pod two-containers-ping --output = yaml output kubectl get pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 apiVersion : v1 kind : Pod metadata : annotations : kubectl.kubernetes.io/last-applied-configuration : | {\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"name\":\"two-containers-ping\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"image\":\"nginx\",\"name\":\"nginx-container\",\"volumeMounts\":[{\"mountPath\":\"/etc/nginx/conf.d\",\"name\":\"shared-data\"}]},{\"image\":\"santos/flask-hello-world\",\"imagePullPolicy\":\"Never\",\"name\":\"flask-test-app\"}],\"restartPolicy\":\"Never\",\"volumes\":[{\"hostPath\":{\"path\":\"/mynginx/\",\"type\":\"Directory\"},\"name\":\"shared-data\"}]}} creationTimestamp : \"2022-06-17T14:38:29Z\" name : two-containers-ping namespace : default resourceVersion : \"52327\" uid : 6dbb4caa-87b3-498a-8c4a-a7aeb76842af spec : containers : - image : nginx imagePullPolicy : Always name : nginx-container resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /etc/nginx/conf.d name : shared-data - mountPath : /var/run/secrets/kubernetes.io/serviceaccount name : kube-api-access-8vpjq readOnly : true - image : santos/flask-hello-world imagePullPolicy : Never name : flask-test-app resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /var/run/secrets/kubernetes.io/serviceaccount name : kube-api-access-8vpjq readOnly : true dnsPolicy : ClusterFirst enableServiceLinks : true nodeName : minikube preemptionPolicy : PreemptLowerPriority priority : 0 restartPolicy : Never schedulerName : default-scheduler securityContext : {} serviceAccount : default serviceAccountName : default terminationGracePeriodSeconds : 30 tolerations : - effect : NoExecute key : node.kubernetes.io/not-ready operator : Exists tolerationSeconds : 300 - effect : NoExecute key : node.kubernetes.io/unreachable operator : Exists tolerationSeconds : 300 volumes : - hostPath : path : /mynginx/ type : Directory name : shared-data - name : kube-api-access-8vpjq projected : defaultMode : 420 sources : - serviceAccountToken : expirationSeconds : 3607 path : token - configMap : items : - key : ca.crt path : ca.crt name : kube-root-ca.crt - downwardAPI : items : - fieldRef : apiVersion : v1 fieldPath : metadata.namespace path : namespace status : conditions : - lastProbeTime : null lastTransitionTime : \"2022-06-17T14:38:29Z\" status : \"True\" type : Initialized - lastProbeTime : null lastTransitionTime : \"2022-06-17T14:38:31Z\" status : \"True\" type : Ready - lastProbeTime : null lastTransitionTime : \"2022-06-17T14:38:31Z\" status : \"True\" type : ContainersReady - lastProbeTime : null lastTransitionTime : \"2022-06-17T14:38:29Z\" status : \"True\" type : PodScheduled containerStatuses : - containerID : docker://0b53b339b7c06b7475739cbde17795ce24f71bc8d7d2e41da07d220162b6475d image : santos/flask-hello-world:latest imageID : docker://sha256:105bc09b0a7e87f7344040742f1fa011e00e7a36bac1f1cabc8a5303ceb1db30 lastState : {} name : flask-test-app ready : true restartCount : 0 started : true state : running : startedAt : \"2022-06-17T14:38:31Z\" - containerID : docker://8175ac7cd60060159888a723b7a36a93486a38e9b6cd4379d7e86fbb4647375d image : nginx:latest imageID : docker-pullable://nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514 lastState : {} name : nginx-container ready : true restartCount : 0 started : true state : running : startedAt : \"2022-06-17T14:38:31Z\" hostIP : 192.168.39.232 phase : Running podIP : 172.17.0.16 podIPs : - ip : 172.17.0.16 qosClass : BestEffort startTime : \"2022-06-17T14:38:29Z\" Como se puede ver los dos containers estan Ready Entrar al container nginx-container $ kubectl exec -it two-containers-ping -c nginx-container -- /bin/bash # root@two-containers-ping:/ Hacer un get a http://localhost/flask_app/ $root @two-containers-ping: curl http://localhost/flask_app # hello word from flask!","title":"Ping desde entre dos containers de un mismo POD"},{"location":"labs_kubernetes/labs3/#conclusion","text":"Containers within a pod share an IP address and port space, and can find each other via localhost s","title":"Conclusion"},{"location":"labs_kubernetes/labs4/","text":"Comunicacion de dos PODS con un service NodePort \u00b6 En este laboratorio se creara un servicio NodePort para que un pod pueda alcanzar a otro usando un SVCHOST . Para este laboratorio se creo una apliacion en flask que se conecta a una base de datos mongo, esta app tiene dos rutas, enviar mensaje, leer mensajes, los mensajes se guardan en la base de datos. #save as app.py from flask import Flask from flask import jsonify from pymongo import MongoClient app = Flask ( __name__ ) # se usa el host del Servicio NodePort client = MongoClient ( \"mongo.default.svc.cluster.local\" , 27017 ) db = client . test_db @app . route ( \"/\" ) def hello (): return \"works\" @app . route ( \"/send_message/<sender>/<to>/<message>\" ) def send_message ( sender , to , message ): return str ( db . messages . insert_one ({ 'to' : to , 'message' : message , 'from' : sender }) . inserted_id ) @app . route ( \"/get_messages/<to>\" ) def get_messages ( to ): messages = list ( db . messages . find ({ \"to\" : to },{ \"_id\" : 0 })) if messages : db . messages . delete_many ({ \"to\" : to }) print ( messages ) return jsonify ( messages ) Escribir el Dockerfile FROM python:3.7 RUN mkdir /app WORKDIR /app/ RUN pip install flask == 2 .0.0 pymongo ADD . /app/ ENV FLASK_APP = app.py ENTRYPOINT [ \"flask\" ] CMD [ \"run\" , \"--host\" , \"0.0.0.0\" , \"-p\" , \"5000\" ] Crear la imagen santos/flask-messages-app-with-mongo a partir del Dockerfile , apuntar al registry de minikube . $ eval $( minikube -p minikube docker-env ) $ docker build . -t santos/flask-messages-app-with-mongo Yaml de creacion de los dos pods usando Deployments # save as ./deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : flask-messages namespace : default spec : selector : matchLabels : app : flask-messages replicas : 1 # template : metadata : labels : app : flask-messages spec : containers : - name : flask-messages image : santos/flask-messages-app-with-mongo imagePullPolicy : Never ports : - containerPort : 5000 --- # Deploy de la db apiVersion : apps/v1 kind : Deployment metadata : name : mongo namespace : default spec : selector : matchLabels : app : mongo replicas : 1 # template : metadata : labels : app : mongo spec : containers : - name : mongo image : mongo imagePullPolicy : IfNotPresent ports : - containerPort : 27017 Ejecutar el deploment.yaml $ kubectl apply -f ./deployment.yaml # deployment.apps/flask-messages created # deployment.apps/mongo created Revisar el estado del cluster $ kubectl get pods # flask-messages-b54fdfbb-2kkrl 1/1 Running 0 40m # mongo-7f4df74f64-mdkgm 1/1 Running 0 40m $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE flask-messages 1 /1 1 1 41m mongo 1 /1 1 1 41m Yaml de creacion de servicios, uno para que la aplicacion pueda acceder a la db con un hostname y otra para poder acceder a la aplicacion desde la maquina local. #save as ./services.yaml apiVersion : v1 kind : Service metadata : name : mongo namespace : default spec : ports : - port : 27017 protocol : TCP targetPort : 27017 selector : app : mongo type : NodePort --- apiVersion : v1 kind : Service metadata : name : flask-messages namespace : default spec : ports : - port : 5000 protocol : TCP targetPort : 5000 selector : app : flask-messages type : NodePort Crear los servicios services.yaml $ kubectl apply -f ./services.yaml # service/mongo created # service/flask-messages created Revisar el estado del cluster $ kubectl get services # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # flask-messages NodePort 10.109.12.181 <none> 5000:31377/TCP 46m # kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 96m # mongo NodePort 10.97.20.81 <none> 27017:31644/TCP 46m 7. Recordar que kubernetes esta corriendo en una maquina virtual, por ende para poder acceder al servicio flask-messages se debe obtener la url de acceso usando minikube $ minikube service flask-messages --url # http://192.168.39.95:31377 Usar la aplicacion # Enviar mensajes curl http://192.168.39.95:31377/new_message/santos/juan/holajuan # 62ae84f9ab010d71976d6c2a curl http://192.168.39.95:31377/new_message/santos/juan/estoyenviandounmensajedesdekubernetes # 62ae8531ab010d71976d6c2b Leer mensajes $ curl http://192.168.39.95:31377/get_messages/juan [ { \"from\" : \"santos\" , \"message\" : \"holajuan\" , \"to\" : \"juan\" }, { \"from\" : \"santos\" , \"message\" : \"estoyenviandounmensajedesdekubernetes\" , \"to\" : \"juan\" } ]","title":"Comunicacion de dos PODS con un service NodePort"},{"location":"labs_kubernetes/labs4/#comunicacion-de-dos-pods-con-un-service-nodeport","text":"En este laboratorio se creara un servicio NodePort para que un pod pueda alcanzar a otro usando un SVCHOST . Para este laboratorio se creo una apliacion en flask que se conecta a una base de datos mongo, esta app tiene dos rutas, enviar mensaje, leer mensajes, los mensajes se guardan en la base de datos. #save as app.py from flask import Flask from flask import jsonify from pymongo import MongoClient app = Flask ( __name__ ) # se usa el host del Servicio NodePort client = MongoClient ( \"mongo.default.svc.cluster.local\" , 27017 ) db = client . test_db @app . route ( \"/\" ) def hello (): return \"works\" @app . route ( \"/send_message/<sender>/<to>/<message>\" ) def send_message ( sender , to , message ): return str ( db . messages . insert_one ({ 'to' : to , 'message' : message , 'from' : sender }) . inserted_id ) @app . route ( \"/get_messages/<to>\" ) def get_messages ( to ): messages = list ( db . messages . find ({ \"to\" : to },{ \"_id\" : 0 })) if messages : db . messages . delete_many ({ \"to\" : to }) print ( messages ) return jsonify ( messages ) Escribir el Dockerfile FROM python:3.7 RUN mkdir /app WORKDIR /app/ RUN pip install flask == 2 .0.0 pymongo ADD . /app/ ENV FLASK_APP = app.py ENTRYPOINT [ \"flask\" ] CMD [ \"run\" , \"--host\" , \"0.0.0.0\" , \"-p\" , \"5000\" ] Crear la imagen santos/flask-messages-app-with-mongo a partir del Dockerfile , apuntar al registry de minikube . $ eval $( minikube -p minikube docker-env ) $ docker build . -t santos/flask-messages-app-with-mongo Yaml de creacion de los dos pods usando Deployments # save as ./deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : flask-messages namespace : default spec : selector : matchLabels : app : flask-messages replicas : 1 # template : metadata : labels : app : flask-messages spec : containers : - name : flask-messages image : santos/flask-messages-app-with-mongo imagePullPolicy : Never ports : - containerPort : 5000 --- # Deploy de la db apiVersion : apps/v1 kind : Deployment metadata : name : mongo namespace : default spec : selector : matchLabels : app : mongo replicas : 1 # template : metadata : labels : app : mongo spec : containers : - name : mongo image : mongo imagePullPolicy : IfNotPresent ports : - containerPort : 27017 Ejecutar el deploment.yaml $ kubectl apply -f ./deployment.yaml # deployment.apps/flask-messages created # deployment.apps/mongo created Revisar el estado del cluster $ kubectl get pods # flask-messages-b54fdfbb-2kkrl 1/1 Running 0 40m # mongo-7f4df74f64-mdkgm 1/1 Running 0 40m $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE flask-messages 1 /1 1 1 41m mongo 1 /1 1 1 41m Yaml de creacion de servicios, uno para que la aplicacion pueda acceder a la db con un hostname y otra para poder acceder a la aplicacion desde la maquina local. #save as ./services.yaml apiVersion : v1 kind : Service metadata : name : mongo namespace : default spec : ports : - port : 27017 protocol : TCP targetPort : 27017 selector : app : mongo type : NodePort --- apiVersion : v1 kind : Service metadata : name : flask-messages namespace : default spec : ports : - port : 5000 protocol : TCP targetPort : 5000 selector : app : flask-messages type : NodePort Crear los servicios services.yaml $ kubectl apply -f ./services.yaml # service/mongo created # service/flask-messages created Revisar el estado del cluster $ kubectl get services # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # flask-messages NodePort 10.109.12.181 <none> 5000:31377/TCP 46m # kubernetes ClusterIP 10.96.0.1 <none> 443/TCP 96m # mongo NodePort 10.97.20.81 <none> 27017:31644/TCP 46m 7. Recordar que kubernetes esta corriendo en una maquina virtual, por ende para poder acceder al servicio flask-messages se debe obtener la url de acceso usando minikube $ minikube service flask-messages --url # http://192.168.39.95:31377 Usar la aplicacion # Enviar mensajes curl http://192.168.39.95:31377/new_message/santos/juan/holajuan # 62ae84f9ab010d71976d6c2a curl http://192.168.39.95:31377/new_message/santos/juan/estoyenviandounmensajedesdekubernetes # 62ae8531ab010d71976d6c2b Leer mensajes $ curl http://192.168.39.95:31377/get_messages/juan [ { \"from\" : \"santos\" , \"message\" : \"holajuan\" , \"to\" : \"juan\" }, { \"from\" : \"santos\" , \"message\" : \"estoyenviandounmensajedesdekubernetes\" , \"to\" : \"juan\" } ]","title":"Comunicacion de dos PODS con un service NodePort"},{"location":"labs_kubernetes/labs5/","text":"Docker Coins en Kubernetes \u00b6 En este laboratorio montara la conocida aplicacion Docker Coins en Kubernetes, especificamente se clono este repositorio Docker Coins Cada microservicio( worker , rng , webui , hasher ) y redis se montara en Pods diferentes. 1. Clonar el repo: git clone https://github.com/platzi/curso-kubernetes # dockercoins projects is in dockercoins folder 2. Como se ve en el diagrama de arquitectura, worker y webui son los unicos microservicios que acceden a otros, se deben modificar las urls de acceso a los otros microservicios, para colocar las urls svc de kubernetes, ya que se usaran servicios NodePort para poder acceder a los microservicios, con los nombres: redis , rng , hasher respectivamente. ### dockercoins/webui/webui.js line 5 - var client = redis.createClient(6379, 'redis'); + var client = redis.createClient(6379, 'redis.default.svc.cluster.local'); ### dockercoins/worker/worker.py line 17 - redis = Redis(\"redis\") + redis = Redis(\"redis.default.svc.cluster.local\") ### dockercoins/worker/worker.py line 21 - r = requests.get(\"http://rng/32\") + r = requests.get(\"http://rng.default.svc.cluster.local/32\") ### dockercoins/worker/worker.py line 26,27,28 - r = requests.post(\"http://hasher/\", - data=data, - headers={\"Content-Type\": \"application/octet-stream\"}) + r = requests.post(\"http://hasher.default.svc.cluster.local/\", + data=data, + headers={\"Content-Type\": \"application/octet-stream\"}) Notar que: En nodejs hay que poner la url sin http:// . En python si hay que colocarlo en las urls que se consumen con la libreria requests . La url de redis no lleva el protocolo: redis.default.svc.cluster.local . Crear las imagenes de docker, por facilidad se usara docker-compose para generar todas las imagenes con un comando. Antes apuntar al registry de minikube $ eval $( minikube -p minikube docker-env ) ## TODO ignore redis build $ docker-compose -f dockercoins/docker-compose.yml build Renombrar las imagenes docker tag dockercoins_worker santos/docker-coins-worker-app docker tag dockercoins_hasher santos/docker-coins-hasher-app docker tag dockercoins_rng:latest santos/docker-coins-rng-app docker tag dockercoins_webui santos/docker-coins-web-ui-app 5. Crear los archivos de depliegue de kubernetes config.yaml apiVersion : v1 kind : ConfigMap metadata : name : redis-configmap namespace : default data : redis.conf : | protected-mode no maxmemory 32mb maxmemory-policy allkeys-lru # cat /usr/local/etc/redis/redis.conf --- apiVersion : v1 kind : ConfigMap metadata : name : redis-sysctl namespace : default data : sysctl.conf : | net.core.somaxconn=511 vm.overcommit_memory=1 net.core.somaxconn=4096 deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : web-ui namespace : default spec : selector : matchLabels : app : web-ui masterapp : dockercoins replicas : 1 # template : metadata : labels : app : web-ui masterapp : dockercoins spec : containers : - name : web-ui image : santos/docker-coins-web-ui-app imagePullPolicy : Never ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : worker namespace : default spec : selector : matchLabels : app : worker masterapp : dockercoins replicas : 1 # template : metadata : labels : app : worker masterapp : dockercoins spec : containers : - name : worker image : santos/docker-coins-worker-app imagePullPolicy : Never # ports: # - containerPort: 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : redis namespace : default spec : selector : matchLabels : app : redis masterapp : dockercoins replicas : 1 # template : metadata : labels : app : redis masterapp : dockercoins spec : containers : - name : redis image : redis imagePullPolicy : IfNotPresent ports : - containerPort : 6379 volumeMounts : - mountPath : /usr/local/etc/redis/redis.conf name : redis-configmap subPath : redis.conf - mountPath : /etc/sysctl.conf name : redis-sysctl subPath : sysctl.conf command : [ \"redis-server\" , \"/usr/local/etc/redis/redis.conf\" ] volumes : - name : redis-configmap configMap : name : redis-configmap - name : redis-sysctl configMap : name : redis-sysctl --- apiVersion : apps/v1 kind : Deployment metadata : name : hasher namespace : default spec : selector : matchLabels : app : hasher masterapp : dockercoins replicas : 1 # template : metadata : labels : app : hasher masterapp : dockercoins spec : containers : - name : hasher image : santos/docker-coins-hasher-app imagePullPolicy : Never ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : rng namespace : default spec : selector : matchLabels : app : rng masterapp : dockercoins replicas : 1 # template : metadata : labels : app : rng masterapp : dockercoins spec : containers : - name : rng image : santos/docker-coins-rng-app imagePullPolicy : Never ports : - containerPort : 80 services.yaml #save as ./services.yaml apiVersion : v1 kind : Service metadata : name : web-ui namespace : default spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : web-ui type : NodePort --- apiVersion : v1 kind : Service metadata : name : redis namespace : default spec : ports : - port : 6379 # protocol: TCP targetPort : 6379 selector : app : redis type : NodePort --- apiVersion : v1 kind : Service metadata : name : hasher namespace : default spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : hasher type : NodePort --- apiVersion : v1 kind : Service metadata : name : rng namespace : default spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : rng type : NodePort Notar que en los deployments hay una etiqueta llamada masterapp con el valor dockercoins, la cual servira para hacer operaciones sobre todos los deployments. 6. Applicar los archivos de configuracion $ kubectl apply -f config.yaml && kubectl apply -f services.yaml \\ && kubectl apply -f deployment.yaml # configmap/redis-configmap created # configmap/redis-sysctl created # service/web-ui created # service/redis created # service/hasher created # service/rng created # deployment.apps/web-ui created # deployment.apps/worker created # deployment.apps/redis created # deployment.apps/hasher created # deployment.apps/rng created # pod/dnsutils created # service/web-ui created # service/redis created # service/hasher created # service/rng created 7. Ver los pods NAME READY STATUS RESTARTS AGE hasher-6fd7cfd7bf-8v86t 1 /1 Running 0 7m51s redis-594c4d767f-8xfrd 1 /1 Running 0 7m51s rng-648c7c46c7-qrn2m 1 /1 Running 0 7m51s web-ui-57b7674785-7cfj2 1 /1 Running 0 7m51s worker-67ff756f89-wvmt4 1 /1 Running 0 7m51s 8. Ver los logs de todos los deployments $ kubectl logs --selector masterapp = dockercoins 1 :C 21 Jun 2022 20 :47:44.841 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1 :C 21 Jun 2022 20 :47:44.842 # Redis version=7.0.2, bits=64, commit=00000000, modified=0, pid=1, just started 1 :C 21 Jun 2022 20 :47:44.842 # Configuration loaded 1 :M 21 Jun 2022 20 :47:44.842 * monotonic clock: POSIX clock_gettime 1 :M 21 Jun 2022 20 :47:44.843 * Running mode = standalone, port = 6379 . 1 :M 21 Jun 2022 20 :47:44.843 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1 :M 21 Jun 2022 20 :47:44.843 # Server initialized 1 :M 21 Jun 2022 20 :47:44.843 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:Coin found: 0c7d1044... INFO:__main__:4 units of work done , updating hash counter INFO:__main__:Coin found: 0fe2be6c... INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1008 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1006 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022:20:51:51 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - WEBUI running on port 80 9. Exponer el servicio NodePort de web-ui con minikube $ minikube service web-ui --url # http://192.168.39.95:31108 10. Abrir el link en el navegador Posibles errores \u00b6 Resolucion de hostnames svc en el microservicio worker o webui , puede ser porque se creo el servicio NodePort de los otros microservicios despues de que se ejecutaran los deplyments, es decir no existian esos hostnames, para esto se puede reiniciar el deployment fallido. $ kubectl rollout restart deployment worker # deployment.apps/worker restarted $ kubectl rollout restart deployment web-ui # deployment.apps/web-ui restarted 2. Connection refused o ENOTFOUND , recordar que: En nodejs hay que poner la url sin http:// . En python si hay que colocar el protocolo en las urls que se consumen con la libreria requests . La url de redis no lleva el protocolo: redis.default.svc.cluster.local . ImageNeverPull , no se apunto al registry de minikube : $ eval $( minikube -p minikube docker-env )","title":"Docker Coins en Kubernetes"},{"location":"labs_kubernetes/labs5/#docker-coins-en-kubernetes","text":"En este laboratorio montara la conocida aplicacion Docker Coins en Kubernetes, especificamente se clono este repositorio Docker Coins Cada microservicio( worker , rng , webui , hasher ) y redis se montara en Pods diferentes. 1. Clonar el repo: git clone https://github.com/platzi/curso-kubernetes # dockercoins projects is in dockercoins folder 2. Como se ve en el diagrama de arquitectura, worker y webui son los unicos microservicios que acceden a otros, se deben modificar las urls de acceso a los otros microservicios, para colocar las urls svc de kubernetes, ya que se usaran servicios NodePort para poder acceder a los microservicios, con los nombres: redis , rng , hasher respectivamente. ### dockercoins/webui/webui.js line 5 - var client = redis.createClient(6379, 'redis'); + var client = redis.createClient(6379, 'redis.default.svc.cluster.local'); ### dockercoins/worker/worker.py line 17 - redis = Redis(\"redis\") + redis = Redis(\"redis.default.svc.cluster.local\") ### dockercoins/worker/worker.py line 21 - r = requests.get(\"http://rng/32\") + r = requests.get(\"http://rng.default.svc.cluster.local/32\") ### dockercoins/worker/worker.py line 26,27,28 - r = requests.post(\"http://hasher/\", - data=data, - headers={\"Content-Type\": \"application/octet-stream\"}) + r = requests.post(\"http://hasher.default.svc.cluster.local/\", + data=data, + headers={\"Content-Type\": \"application/octet-stream\"}) Notar que: En nodejs hay que poner la url sin http:// . En python si hay que colocarlo en las urls que se consumen con la libreria requests . La url de redis no lleva el protocolo: redis.default.svc.cluster.local . Crear las imagenes de docker, por facilidad se usara docker-compose para generar todas las imagenes con un comando. Antes apuntar al registry de minikube $ eval $( minikube -p minikube docker-env ) ## TODO ignore redis build $ docker-compose -f dockercoins/docker-compose.yml build Renombrar las imagenes docker tag dockercoins_worker santos/docker-coins-worker-app docker tag dockercoins_hasher santos/docker-coins-hasher-app docker tag dockercoins_rng:latest santos/docker-coins-rng-app docker tag dockercoins_webui santos/docker-coins-web-ui-app 5. Crear los archivos de depliegue de kubernetes config.yaml apiVersion : v1 kind : ConfigMap metadata : name : redis-configmap namespace : default data : redis.conf : | protected-mode no maxmemory 32mb maxmemory-policy allkeys-lru # cat /usr/local/etc/redis/redis.conf --- apiVersion : v1 kind : ConfigMap metadata : name : redis-sysctl namespace : default data : sysctl.conf : | net.core.somaxconn=511 vm.overcommit_memory=1 net.core.somaxconn=4096 deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : web-ui namespace : default spec : selector : matchLabels : app : web-ui masterapp : dockercoins replicas : 1 # template : metadata : labels : app : web-ui masterapp : dockercoins spec : containers : - name : web-ui image : santos/docker-coins-web-ui-app imagePullPolicy : Never ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : worker namespace : default spec : selector : matchLabels : app : worker masterapp : dockercoins replicas : 1 # template : metadata : labels : app : worker masterapp : dockercoins spec : containers : - name : worker image : santos/docker-coins-worker-app imagePullPolicy : Never # ports: # - containerPort: 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : redis namespace : default spec : selector : matchLabels : app : redis masterapp : dockercoins replicas : 1 # template : metadata : labels : app : redis masterapp : dockercoins spec : containers : - name : redis image : redis imagePullPolicy : IfNotPresent ports : - containerPort : 6379 volumeMounts : - mountPath : /usr/local/etc/redis/redis.conf name : redis-configmap subPath : redis.conf - mountPath : /etc/sysctl.conf name : redis-sysctl subPath : sysctl.conf command : [ \"redis-server\" , \"/usr/local/etc/redis/redis.conf\" ] volumes : - name : redis-configmap configMap : name : redis-configmap - name : redis-sysctl configMap : name : redis-sysctl --- apiVersion : apps/v1 kind : Deployment metadata : name : hasher namespace : default spec : selector : matchLabels : app : hasher masterapp : dockercoins replicas : 1 # template : metadata : labels : app : hasher masterapp : dockercoins spec : containers : - name : hasher image : santos/docker-coins-hasher-app imagePullPolicy : Never ports : - containerPort : 80 --- apiVersion : apps/v1 kind : Deployment metadata : name : rng namespace : default spec : selector : matchLabels : app : rng masterapp : dockercoins replicas : 1 # template : metadata : labels : app : rng masterapp : dockercoins spec : containers : - name : rng image : santos/docker-coins-rng-app imagePullPolicy : Never ports : - containerPort : 80 services.yaml #save as ./services.yaml apiVersion : v1 kind : Service metadata : name : web-ui namespace : default spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : web-ui type : NodePort --- apiVersion : v1 kind : Service metadata : name : redis namespace : default spec : ports : - port : 6379 # protocol: TCP targetPort : 6379 selector : app : redis type : NodePort --- apiVersion : v1 kind : Service metadata : name : hasher namespace : default spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : hasher type : NodePort --- apiVersion : v1 kind : Service metadata : name : rng namespace : default spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : app : rng type : NodePort Notar que en los deployments hay una etiqueta llamada masterapp con el valor dockercoins, la cual servira para hacer operaciones sobre todos los deployments. 6. Applicar los archivos de configuracion $ kubectl apply -f config.yaml && kubectl apply -f services.yaml \\ && kubectl apply -f deployment.yaml # configmap/redis-configmap created # configmap/redis-sysctl created # service/web-ui created # service/redis created # service/hasher created # service/rng created # deployment.apps/web-ui created # deployment.apps/worker created # deployment.apps/redis created # deployment.apps/hasher created # deployment.apps/rng created # pod/dnsutils created # service/web-ui created # service/redis created # service/hasher created # service/rng created 7. Ver los pods NAME READY STATUS RESTARTS AGE hasher-6fd7cfd7bf-8v86t 1 /1 Running 0 7m51s redis-594c4d767f-8xfrd 1 /1 Running 0 7m51s rng-648c7c46c7-qrn2m 1 /1 Running 0 7m51s web-ui-57b7674785-7cfj2 1 /1 Running 0 7m51s worker-67ff756f89-wvmt4 1 /1 Running 0 7m51s 8. Ver los logs de todos los deployments $ kubectl logs --selector masterapp = dockercoins 1 :C 21 Jun 2022 20 :47:44.841 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1 :C 21 Jun 2022 20 :47:44.842 # Redis version=7.0.2, bits=64, commit=00000000, modified=0, pid=1, just started 1 :C 21 Jun 2022 20 :47:44.842 # Configuration loaded 1 :M 21 Jun 2022 20 :47:44.842 * monotonic clock: POSIX clock_gettime 1 :M 21 Jun 2022 20 :47:44.843 * Running mode = standalone, port = 6379 . 1 :M 21 Jun 2022 20 :47:44.843 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1 :M 21 Jun 2022 20 :47:44.843 # Server initialized 1 :M 21 Jun 2022 20 :47:44.843 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:Coin found: 0c7d1044... INFO:__main__:4 units of work done , updating hash counter INFO:__main__:Coin found: 0fe2be6c... INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1008 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1006 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022:20:51:51 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - WEBUI running on port 80 9. Exponer el servicio NodePort de web-ui con minikube $ minikube service web-ui --url # http://192.168.39.95:31108 10. Abrir el link en el navegador","title":"Docker Coins en Kubernetes"},{"location":"labs_kubernetes/labs5/#posibles-errores","text":"Resolucion de hostnames svc en el microservicio worker o webui , puede ser porque se creo el servicio NodePort de los otros microservicios despues de que se ejecutaran los deplyments, es decir no existian esos hostnames, para esto se puede reiniciar el deployment fallido. $ kubectl rollout restart deployment worker # deployment.apps/worker restarted $ kubectl rollout restart deployment web-ui # deployment.apps/web-ui restarted 2. Connection refused o ENOTFOUND , recordar que: En nodejs hay que poner la url sin http:// . En python si hay que colocar el protocolo en las urls que se consumen con la libreria requests . La url de redis no lleva el protocolo: redis.default.svc.cluster.local . ImageNeverPull , no se apunto al registry de minikube : $ eval $( minikube -p minikube docker-env )","title":"Posibles errores"},{"location":"labs_kubernetes/labs6/","text":"Azure Kubernetes Service \u00b6 En este lab se mostrara como levantar un container registry y un cluster en Azure, y luego se desplegara la aplicacion Docker Coins . Crear los recursos en Azure \u00b6 Crear el Container Registry para subir las imagenes de docker: az acr create --resource-group solok8s \\ --name solok8sregistry --sku Basic 2. Crear un Cluster az aks create --resource-group solok8s --name dockercoins --node-count 1 --generate-ssh-keys 3. Realcionar el Cluster al ACR Creado en el paso 1 az aks update -n dockercoins -g solok8s --attach-acr solok8sregistry Con estos comandos ya se tiene un cluster y un acr para empezar a desplegar aplicaciones Ahora bien para poder manipular estos dos desde la terminar se deben ejecutar los siguientes comandos Acceder a los recursos desde la terminal \u00b6 Login en el ACR , para poder publicar imagenes az acr login --name solok8sregistry Obtener el nombre dominio del nuevo container registry. az acr show --name solok8sregistry --query loginServer --output table # Result # -------------------------- # solok8sregistry.azurecr.io Para subir una imagen al registry se debe nombrar de la siguiente forma <registry>/<name>:<tag> , en este caso a modo de ejemplo solok8sregistry.azurecr.io/regis:latest Obterner las credenciales de acceso al Cluster az aks get-credentials --resource-group solok8s --name dockercoins Ya se puede acceder al cluster con kubectl kubectl get pods Subir Docker Coins al Nuevo Cluster \u00b6 Se deben seguir los mismos pasos que con minikube, pero previamente estando autenticado en el ACR y el Cluster . 1. Renombrar las imagenes que se quieren subir al nuevo ACR docker tag dockercoins_worker solok8sregistry.azurecr.io/docker-coins-worker-app docker tag dockercoins_hasher solok8sregistry.azurecr.io/docker-coins-hasher-app docker tag dockercoins_rng:latest solok8sregistry.azurecr.io/docker-coins-rng-app docker tag dockercoins_webui solok8sregistry.azurecr.io/docker-coins-web-ui-app 2. publicar las imagenes en el ACR docker push solok8sregistry.azurecr.io/docker-coins-worker-app docker push solok8sregistry.azurecr.io/docker-coins-hasher-app docker push solok8sregistry.azurecr.io/docker-coins-rng-app docker push solok8sregistry.azurecr.io/docker-coins-web-ui-app 3. Se pueden usar los mismos archivos yaml que se usaron con minikube. Applicar los archivos de configuracion $ kubectl apply -f config.yaml && kubectl apply -f services.yaml \\ && kubectl apply -f deployment.yaml # configmap/redis-configmap created # configmap/redis-sysctl created # service/web-ui created # service/redis created # service/hasher created # service/rng created # deployment.apps/web-ui created # deployment.apps/worker created # deployment.apps/redis created # deployment.apps/hasher created # deployment.apps/rng created # pod/dnsutils created # service/web-ui created # service/redis created # service/hasher created # service/rng created 4. Ver los pods NAME READY STATUS RESTARTS AGE hasher-6fd7cfd7bf-8v86t 1 /1 Running 0 7m51s redis-594c4d767f-8xfrd 1 /1 Running 0 7m51s rng-648c7c46c7-qrn2m 1 /1 Running 0 7m51s web-ui-57b7674785-7cfj2 1 /1 Running 0 7m51s worker-67ff756f89-wvmt4 1 /1 Running 0 7m51s 5. Ver los logs de todos los deployments $ kubectl logs --selector masterapp = dockercoins 1 :C 21 Jun 2022 20 :47:44.841 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1 :C 21 Jun 2022 20 :47:44.842 # Redis version=7.0.2, bits=64, commit=00000000, modified=0, pid=1, just started 1 :C 21 Jun 2022 20 :47:44.842 # Configuration loaded 1 :M 21 Jun 2022 20 :47:44.842 * monotonic clock: POSIX clock_gettime 1 :M 21 Jun 2022 20 :47:44.843 * Running mode = standalone, port = 6379 . 1 :M 21 Jun 2022 20 :47:44.843 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1 :M 21 Jun 2022 20 :47:44.843 # Server initialized 1 :M 21 Jun 2022 20 :47:44.843 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:Coin found: 0c7d1044... INFO:__main__:4 units of work done , updating hash counter INFO:__main__:Coin found: 0fe2be6c... INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1008 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1006 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022:20:51:51 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - WEBUI running on port 80 6. Hacer un portforward al service web-ui en el puerto 80 kubectl port-forward svc/web-ui 8011:80 7. Abrir en el navegador la url localhost:8011","title":"Azure Kubernetes Service"},{"location":"labs_kubernetes/labs6/#azure-kubernetes-service","text":"En este lab se mostrara como levantar un container registry y un cluster en Azure, y luego se desplegara la aplicacion Docker Coins .","title":"Azure Kubernetes Service"},{"location":"labs_kubernetes/labs6/#crear-los-recursos-en-azure","text":"Crear el Container Registry para subir las imagenes de docker: az acr create --resource-group solok8s \\ --name solok8sregistry --sku Basic 2. Crear un Cluster az aks create --resource-group solok8s --name dockercoins --node-count 1 --generate-ssh-keys 3. Realcionar el Cluster al ACR Creado en el paso 1 az aks update -n dockercoins -g solok8s --attach-acr solok8sregistry Con estos comandos ya se tiene un cluster y un acr para empezar a desplegar aplicaciones Ahora bien para poder manipular estos dos desde la terminar se deben ejecutar los siguientes comandos","title":"Crear los recursos en Azure"},{"location":"labs_kubernetes/labs6/#acceder-a-los-recursos-desde-la-terminal","text":"Login en el ACR , para poder publicar imagenes az acr login --name solok8sregistry Obtener el nombre dominio del nuevo container registry. az acr show --name solok8sregistry --query loginServer --output table # Result # -------------------------- # solok8sregistry.azurecr.io Para subir una imagen al registry se debe nombrar de la siguiente forma <registry>/<name>:<tag> , en este caso a modo de ejemplo solok8sregistry.azurecr.io/regis:latest Obterner las credenciales de acceso al Cluster az aks get-credentials --resource-group solok8s --name dockercoins Ya se puede acceder al cluster con kubectl kubectl get pods","title":"Acceder a los recursos desde la terminal"},{"location":"labs_kubernetes/labs6/#subir-docker-coins-al-nuevo-cluster","text":"Se deben seguir los mismos pasos que con minikube, pero previamente estando autenticado en el ACR y el Cluster . 1. Renombrar las imagenes que se quieren subir al nuevo ACR docker tag dockercoins_worker solok8sregistry.azurecr.io/docker-coins-worker-app docker tag dockercoins_hasher solok8sregistry.azurecr.io/docker-coins-hasher-app docker tag dockercoins_rng:latest solok8sregistry.azurecr.io/docker-coins-rng-app docker tag dockercoins_webui solok8sregistry.azurecr.io/docker-coins-web-ui-app 2. publicar las imagenes en el ACR docker push solok8sregistry.azurecr.io/docker-coins-worker-app docker push solok8sregistry.azurecr.io/docker-coins-hasher-app docker push solok8sregistry.azurecr.io/docker-coins-rng-app docker push solok8sregistry.azurecr.io/docker-coins-web-ui-app 3. Se pueden usar los mismos archivos yaml que se usaron con minikube. Applicar los archivos de configuracion $ kubectl apply -f config.yaml && kubectl apply -f services.yaml \\ && kubectl apply -f deployment.yaml # configmap/redis-configmap created # configmap/redis-sysctl created # service/web-ui created # service/redis created # service/hasher created # service/rng created # deployment.apps/web-ui created # deployment.apps/worker created # deployment.apps/redis created # deployment.apps/hasher created # deployment.apps/rng created # pod/dnsutils created # service/web-ui created # service/redis created # service/hasher created # service/rng created 4. Ver los pods NAME READY STATUS RESTARTS AGE hasher-6fd7cfd7bf-8v86t 1 /1 Running 0 7m51s redis-594c4d767f-8xfrd 1 /1 Running 0 7m51s rng-648c7c46c7-qrn2m 1 /1 Running 0 7m51s web-ui-57b7674785-7cfj2 1 /1 Running 0 7m51s worker-67ff756f89-wvmt4 1 /1 Running 0 7m51s 5. Ver los logs de todos los deployments $ kubectl logs --selector masterapp = dockercoins 1 :C 21 Jun 2022 20 :47:44.841 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 1 :C 21 Jun 2022 20 :47:44.842 # Redis version=7.0.2, bits=64, commit=00000000, modified=0, pid=1, just started 1 :C 21 Jun 2022 20 :47:44.842 # Configuration loaded 1 :M 21 Jun 2022 20 :47:44.842 * monotonic clock: POSIX clock_gettime 1 :M 21 Jun 2022 20 :47:44.843 * Running mode = standalone, port = 6379 . 1 :M 21 Jun 2022 20 :47:44.843 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1 :M 21 Jun 2022 20 :47:44.843 # Server initialized 1 :M 21 Jun 2022 20 :47:44.843 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:Coin found: 0c7d1044... INFO:__main__:4 units of work done , updating hash counter INFO:__main__:Coin found: 0fe2be6c... INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter INFO:__main__:4 units of work done , updating hash counter 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1008 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1006 172 .17.0.1 - - [ 21 /Jun/2022:20:51:50 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022:20:51:51 +0000 ] \"POST / HTTP/1.1\" 200 64 0 .1007 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:03 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - 172 .17.0.1 - - [ 21 /Jun/2022 20 :52:04 ] \"GET /32 HTTP/1.1\" 200 - WEBUI running on port 80 6. Hacer un portforward al service web-ui en el puerto 80 kubectl port-forward svc/web-ui 8011:80 7. Abrir en el navegador la url localhost:8011","title":"Subir Docker Coins al Nuevo Cluster"},{"location":"sre_tools/rg_massive_tags/","text":"AZURE MASSIVE TAGGING CLI TOOL \u00b6 Esta herramienta tiene el objetivo de crear tags a los grupos de recursos listados en un excel name tag 1 tag 2 .... tag n rg 1 val1 va2 val3 rg 2 val1 va2 val3 rg 3 val1 va2 val3 Requisitos \u00b6 Python3 Librer\u00edas de python: Las librer\u00edas de python requeridas est\u00e1n listadas en el archivo requirements.txt Requisitos de autenticaci\u00f3n: ( opcional ) Un archivo con los accesos de un service principal, con la siguiente estructura. Lo necesitas solo si quieres autenticarte en azure de esta forma. { \"clientId\" : \"52311**************69cac152\" , \"clientSecret\" : \"K_As23a**********SADAWds\" , \"tenantId\" : \"4623*************b0d39089f788\" } ( opcional ) AZ CLI Con sesi\u00f3n activa, lo necesitas solo si quieres autenticarte por este medio. C\u00f3mo funciona la herramienta \u00b6 La herramienta funciona en dos partes, esto con el objetivo de poder validar los tags finales que se enviaran al cloud antes, y disminuir el riesgo de errores. A continuaci\u00f3n un diagrama de flujo de alto nivel del flujo de la aplicaci\u00f3n. Paso 1 Make Tags \u00b6 El primer paso es ejecutar make_tags en este paso se recorre el excel y por cada grupo de recursos se consultan sus tags en el cloud y se mezclan con los tags del excel, para ser guardados posteriormente en un archivo nombrado con un identificador \u00fanico migration_id con el siguiente formato: migrations/tags-{{migration_id}}.json . La salida de este paso es el migration_id el cual usar\u00e1s en el siguiente paso. Paso 2 Send Tags \u00b6 El segundo paso es ejecutar send_tags en este paso se debe especificar el migration_id del paso anterior. Importante Recuerda que antes debes validar el archivo con los tags finales que se enviar\u00e1n al cloud y verificar que es correcto, en caso contrario trata de identificar el error, solucionarlo y crear una nueva migraci\u00f3n. C\u00f3mo ejecutar la herramienta \u00b6 Hay dos formas de ejecutar la herramienta. Argumentos como par\u00e1metro de la CLI \u00b6 Aqui un peque\u00f1o ejemplo: # Paso 1 : Generar archivo de migraci\u00f3n python manage.py make_tags --excel-path ./inputs/TAGULTI.xlsx --excel-sheet Hoja1 --tags TRIBU:Tribu DATE:Date ID:Mail OWNER:Owner --subscription-id b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa --az-manager-type SDK --az-credentials-file .az_credentials.json Making Tags 20220330102531 See logs in logs/20220330102531.log Progress |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100.0% Complete See made tags in tags-20220330102531.json file Paso 2: Enviar Tags a AZURE(Especificar el id de la migraci\u00f3n generada en el paso anterior *migration_id*) informatica@rami802288-11:~/dev/p$ python manage.py send_tags --subscription-id b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa --azure-manager-type SDK --migration-id 20220330102531 Sending Tags 20220330102531 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588-| 100% Complete Para ver la documentaci\u00f3n de cada comando env\u00eda la opci\u00f3n --help python manage.y make_tags --help # Para ver la doc de MAKE TAGS python manage.y send_tags --help # Para ver la doc de SEND TAGS Pasar argumentos en un JSON de la CLI \u00b6 Esta forma de ejecutar la CLI es m\u00e1s organizada y funciona de la siguiente forma: python manage.py make_tags ./make_tags_params.json Making Tags 20220330102531 See logs in logs/20220330102531.log Progress |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100.0% Complete See made tags in tags-20220330102531.json file python manage.py send ./send_tags_params.json Sending Tags 20220330102531 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588-| 100% Complete El archivo make_tags_params.json debe tener la siguiente estructura: { \"excel_path\" : \"./inputs/TAGULTI.xlsx\" , \"excel_sheet\" : \"Hoja1\" , \"tags\" :{ //\"Nombre de la columna en excel\": \"C\u00f3mo va a quedar en el cloud\" \"TRIBU\" : \"Tribu\" , // Columna Excel : Tag Name \"DATE\" : \"Date\" , \"ID\" : \"Mail\" , \"OWNER\" : \"Owner\" }, \"subscription_id\" : \"b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa\" , \"az_manager_type\" : \"API\" , // API / SDK \"az_auth_method\" : \"TOKEN\" , //TOKEN / CLI \"az_credentials_file\" : \".az_credentials.json\" , // Si eligi\u00f3 TOKEN en auth_method, debe especificar el archivo con las credenciales(Es un service principal) \"extra_fields\" : [ \"location\" ] // Opcional } El archivo send_tags_params.json debe tener la siguiente estructura: { \"migration_id\" : \"20220329143907\" , // El id de los tags que quiere enviar a azure \"subscription_id\" : \"b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa\" , \"az_manager_type\" : \"API\" , // API / SDK \"az_auth_method\" : \"TOKEN\" , //TOKEN / CLI \"az_credentials_file\" : \".az_credentials.json\" , // Si eligi\u00f3 TOKEN en auth_method, debe especificar el archivo con las credenciales(Es un service principal) \"enter_step\" : true // Opcional }","title":"AZURE MASSIVE TAGGING CLI TOOL"},{"location":"sre_tools/rg_massive_tags/#azure-massive-tagging-cli-tool","text":"Esta herramienta tiene el objetivo de crear tags a los grupos de recursos listados en un excel name tag 1 tag 2 .... tag n rg 1 val1 va2 val3 rg 2 val1 va2 val3 rg 3 val1 va2 val3","title":"AZURE MASSIVE TAGGING CLI TOOL"},{"location":"sre_tools/rg_massive_tags/#requisitos","text":"Python3 Librer\u00edas de python: Las librer\u00edas de python requeridas est\u00e1n listadas en el archivo requirements.txt Requisitos de autenticaci\u00f3n: ( opcional ) Un archivo con los accesos de un service principal, con la siguiente estructura. Lo necesitas solo si quieres autenticarte en azure de esta forma. { \"clientId\" : \"52311**************69cac152\" , \"clientSecret\" : \"K_As23a**********SADAWds\" , \"tenantId\" : \"4623*************b0d39089f788\" } ( opcional ) AZ CLI Con sesi\u00f3n activa, lo necesitas solo si quieres autenticarte por este medio.","title":"Requisitos"},{"location":"sre_tools/rg_massive_tags/#como-funciona-la-herramienta","text":"La herramienta funciona en dos partes, esto con el objetivo de poder validar los tags finales que se enviaran al cloud antes, y disminuir el riesgo de errores. A continuaci\u00f3n un diagrama de flujo de alto nivel del flujo de la aplicaci\u00f3n.","title":"C\u00f3mo funciona la herramienta"},{"location":"sre_tools/rg_massive_tags/#paso-1-make-tags","text":"El primer paso es ejecutar make_tags en este paso se recorre el excel y por cada grupo de recursos se consultan sus tags en el cloud y se mezclan con los tags del excel, para ser guardados posteriormente en un archivo nombrado con un identificador \u00fanico migration_id con el siguiente formato: migrations/tags-{{migration_id}}.json . La salida de este paso es el migration_id el cual usar\u00e1s en el siguiente paso.","title":"Paso 1 Make Tags"},{"location":"sre_tools/rg_massive_tags/#paso-2-send-tags","text":"El segundo paso es ejecutar send_tags en este paso se debe especificar el migration_id del paso anterior. Importante Recuerda que antes debes validar el archivo con los tags finales que se enviar\u00e1n al cloud y verificar que es correcto, en caso contrario trata de identificar el error, solucionarlo y crear una nueva migraci\u00f3n.","title":"Paso 2 Send Tags"},{"location":"sre_tools/rg_massive_tags/#como-ejecutar-la-herramienta","text":"Hay dos formas de ejecutar la herramienta.","title":"C\u00f3mo ejecutar la herramienta"},{"location":"sre_tools/rg_massive_tags/#argumentos-como-parametro-de-la-cli","text":"Aqui un peque\u00f1o ejemplo: # Paso 1 : Generar archivo de migraci\u00f3n python manage.py make_tags --excel-path ./inputs/TAGULTI.xlsx --excel-sheet Hoja1 --tags TRIBU:Tribu DATE:Date ID:Mail OWNER:Owner --subscription-id b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa --az-manager-type SDK --az-credentials-file .az_credentials.json Making Tags 20220330102531 See logs in logs/20220330102531.log Progress |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100.0% Complete See made tags in tags-20220330102531.json file Paso 2: Enviar Tags a AZURE(Especificar el id de la migraci\u00f3n generada en el paso anterior *migration_id*) informatica@rami802288-11:~/dev/p$ python manage.py send_tags --subscription-id b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa --azure-manager-type SDK --migration-id 20220330102531 Sending Tags 20220330102531 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588-| 100% Complete Para ver la documentaci\u00f3n de cada comando env\u00eda la opci\u00f3n --help python manage.y make_tags --help # Para ver la doc de MAKE TAGS python manage.y send_tags --help # Para ver la doc de SEND TAGS","title":"Argumentos como par\u00e1metro de la CLI"},{"location":"sre_tools/rg_massive_tags/#pasar-argumentos-en-un-json-de-la-cli","text":"Esta forma de ejecutar la CLI es m\u00e1s organizada y funciona de la siguiente forma: python manage.py make_tags ./make_tags_params.json Making Tags 20220330102531 See logs in logs/20220330102531.log Progress |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100.0% Complete See made tags in tags-20220330102531.json file python manage.py send ./send_tags_params.json Sending Tags 20220330102531 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588-| 100% Complete El archivo make_tags_params.json debe tener la siguiente estructura: { \"excel_path\" : \"./inputs/TAGULTI.xlsx\" , \"excel_sheet\" : \"Hoja1\" , \"tags\" :{ //\"Nombre de la columna en excel\": \"C\u00f3mo va a quedar en el cloud\" \"TRIBU\" : \"Tribu\" , // Columna Excel : Tag Name \"DATE\" : \"Date\" , \"ID\" : \"Mail\" , \"OWNER\" : \"Owner\" }, \"subscription_id\" : \"b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa\" , \"az_manager_type\" : \"API\" , // API / SDK \"az_auth_method\" : \"TOKEN\" , //TOKEN / CLI \"az_credentials_file\" : \".az_credentials.json\" , // Si eligi\u00f3 TOKEN en auth_method, debe especificar el archivo con las credenciales(Es un service principal) \"extra_fields\" : [ \"location\" ] // Opcional } El archivo send_tags_params.json debe tener la siguiente estructura: { \"migration_id\" : \"20220329143907\" , // El id de los tags que quiere enviar a azure \"subscription_id\" : \"b2fd9f8c-0ed5-4f6e-9c93-75ae90718afa\" , \"az_manager_type\" : \"API\" , // API / SDK \"az_auth_method\" : \"TOKEN\" , //TOKEN / CLI \"az_credentials_file\" : \".az_credentials.json\" , // Si eligi\u00f3 TOKEN en auth_method, debe especificar el archivo con las credenciales(Es un service principal) \"enter_step\" : true // Opcional }","title":"Pasar argumentos en un JSON de la CLI"},{"location":"terraform/","text":"Terraform \u00b6 Siigo tiene la meta de tener toda su infraestructura desplegada con terraform, hasta el momento va un aproximado del 40% y esta se maneja en dos repositorios en azure devops. Repositorios \u00b6 Infrastructure Modules \u00b6 Este repositorio tiene las plantillas para desplegar los diferentes servicios con los est\u00e1ndares ya incorporados. Se manejan TAGS para el versionamiento de cada m\u00f3dulo siguiendo el siguiente est\u00e1ndar {modulename}v{version-No} , algunos ejemplos son: aks-istio-sp-v0.0.1 , aks-nodepools-v0.0.9 Por lo general cada m\u00f3dulo tiene un README.MD donde explica los cambios m\u00e1s importantes en cada versi\u00f3n. La estructura de carpetas est\u00e1 divida de la siguiente forma: Proveedor/Servicio . La rama principal es dev , se utiliza el esquema de Git Flow , m\u00e1s adelante se explica el proceso. Ver el repositorio en el siguiente link Repositorio IAC \u00b6 En este repositorio est\u00e1 escrita la infraestructura de SIIGO , implementando las diferentes plantillas del repo Infrastructure Modules . La estructura de carpetas del repositorio est\u00e1 dividida as\u00ed Pais/Ambiente/Servicio . La rama principal de este repositorio es infrastructure-code . Ver el repositorio en el siguiente link Gesti\u00f3n de cambios \u00b6 La gesti\u00f3n de cambios en los repositorios se hace usando la metodolog\u00eda Git Flow , estos cambio se dividen en 2 tipos Actualizaci\u00f3n de los m\u00f3dulos de Infraestructura Modules \u00b6 Por lo general los cambios en este repositorio tienen que ver con utilizar nuevas versiones de diferentes tecnolog\u00edas, solucionar bugs, registrar namespaces, actualizar o agregar m\u00f3dulos, etc. Todo cambio sustancial en este repositorio es Taggeado con el objetivo de poder referenciar desde el repo de IaC . Mas sobre GitFlow sobre este repo en link Cambios en IaC \u00b6 Los cambios en este repositorio tienen que ver con cambios en la infraestructura, esta infraestructura es creada implementando los m\u00f3dulos del repo de infrastructure-modules . Mas sobre GitFlow sobre este repo en link","title":"Terraform"},{"location":"terraform/#terraform","text":"Siigo tiene la meta de tener toda su infraestructura desplegada con terraform, hasta el momento va un aproximado del 40% y esta se maneja en dos repositorios en azure devops.","title":"Terraform"},{"location":"terraform/#repositorios","text":"","title":"Repositorios"},{"location":"terraform/#infrastructure-modules","text":"Este repositorio tiene las plantillas para desplegar los diferentes servicios con los est\u00e1ndares ya incorporados. Se manejan TAGS para el versionamiento de cada m\u00f3dulo siguiendo el siguiente est\u00e1ndar {modulename}v{version-No} , algunos ejemplos son: aks-istio-sp-v0.0.1 , aks-nodepools-v0.0.9 Por lo general cada m\u00f3dulo tiene un README.MD donde explica los cambios m\u00e1s importantes en cada versi\u00f3n. La estructura de carpetas est\u00e1 divida de la siguiente forma: Proveedor/Servicio . La rama principal es dev , se utiliza el esquema de Git Flow , m\u00e1s adelante se explica el proceso. Ver el repositorio en el siguiente link","title":"Infrastructure Modules"},{"location":"terraform/#repositorio-iac","text":"En este repositorio est\u00e1 escrita la infraestructura de SIIGO , implementando las diferentes plantillas del repo Infrastructure Modules . La estructura de carpetas del repositorio est\u00e1 dividida as\u00ed Pais/Ambiente/Servicio . La rama principal de este repositorio es infrastructure-code . Ver el repositorio en el siguiente link","title":"Repositorio IAC"},{"location":"terraform/#gestion-de-cambios","text":"La gesti\u00f3n de cambios en los repositorios se hace usando la metodolog\u00eda Git Flow , estos cambio se dividen en 2 tipos","title":"Gesti\u00f3n de cambios"},{"location":"terraform/#actualizacion-de-los-modulos-de-infraestructura-modules","text":"Por lo general los cambios en este repositorio tienen que ver con utilizar nuevas versiones de diferentes tecnolog\u00edas, solucionar bugs, registrar namespaces, actualizar o agregar m\u00f3dulos, etc. Todo cambio sustancial en este repositorio es Taggeado con el objetivo de poder referenciar desde el repo de IaC . Mas sobre GitFlow sobre este repo en link","title":"Actualizaci\u00f3n de los m\u00f3dulos de  Infraestructura Modules"},{"location":"terraform/#cambios-en-iac","text":"Los cambios en este repositorio tienen que ver con cambios en la infraestructura, esta infraestructura es creada implementando los m\u00f3dulos del repo de infrastructure-modules . Mas sobre GitFlow sobre este repo en link","title":"Cambios en IaC"},{"location":"terraform/gitflow/","text":"Git Flow para los repos de Terraform \u00b6 Git flow es una metodolog\u00eda de trabajo que facilita la integraci\u00f3n de cambios de equipos grandes de desarrollo, basada en el divisi\u00f3n de las distintas etapas de producci\u00f3n de software en distintas ramas del repositorio: Master: En la rama m\u00e1ster se encuentran las releases estables de nuestro software. Esta es la rama que un usuario t\u00edpico se descarga para usar nuestro software, por lo que todo lo que hay en esta rama deber\u00eda ser funcional. Sin embargo, puede que las \u00faltimas mejoras introducidas en el software no est\u00e9n disponibles todav\u00eda en esta rama. Develop: En esta rama surge de la \u00faltima release de m\u00e1ster. En ella se van integrando todas las nuevas caracter\u00edsticas hasta la siguiente release. Feature-X: Cada nueva mejora o caracter\u00edstica que vayamos a introducir en nuestro software tendr\u00e1 una rama que contendr\u00e1 su desarrollo. Las ramas de feature salen de la rama develop y una vez completado el desarrollo de la mejora, se vuelven a integrar en el develop. Release-X: Las ramas de release se crean cuando se va a publicar la siguiente versi\u00f3n del software y surgen de la rama develop . En estas ramas, el desarrollo de nuevas caracter\u00edsticas se congela, y se trabaja en arreglar bugs y generar documentaci\u00f3n. Una vez listo para la publicaci\u00f3n, se integra en m\u00e1ster y se etiqueta con el n\u00famero de versi\u00f3n correspondiente. Se integran tambi\u00e9n con develop, ya que su contenido ha podido cambiar debido a nuevas mejoras. Hotfix-X: Si nuestro c\u00f3digo contiene bugs cr\u00edticos que es necesario parchear de manera inmediata, es posible crear una rama hotfix a partir de la publicaci\u00f3n correspondiente en la rama master. Esta rama contendr\u00e1 \u00fanicamente los cambios que haya que realizar para parchear el bug. Una vez arreglado, se integrar\u00e1 en master, con su etiqueta de versi\u00f3n correspondiente y en develop. Flujo de trabajo \u00b6 En este momento nosotros hacemos uso de un esquema reducido de esta metodolog\u00eda teniendo solamente de las ramas de caracter\u00edsticas, y ramas primarias. A continuaci\u00f3n hacemos una explicaci\u00f3n corta de c\u00f3mo aplicar la metodolog\u00eda. Nota \u00b6 Se hace una documentaci\u00f3n por cada repositorio para hacerlo m\u00e1s sencillo.","title":"Git Flow para los repos de Terraform"},{"location":"terraform/gitflow/#git-flow-para-los-repos-de-terraform","text":"Git flow es una metodolog\u00eda de trabajo que facilita la integraci\u00f3n de cambios de equipos grandes de desarrollo, basada en el divisi\u00f3n de las distintas etapas de producci\u00f3n de software en distintas ramas del repositorio: Master: En la rama m\u00e1ster se encuentran las releases estables de nuestro software. Esta es la rama que un usuario t\u00edpico se descarga para usar nuestro software, por lo que todo lo que hay en esta rama deber\u00eda ser funcional. Sin embargo, puede que las \u00faltimas mejoras introducidas en el software no est\u00e9n disponibles todav\u00eda en esta rama. Develop: En esta rama surge de la \u00faltima release de m\u00e1ster. En ella se van integrando todas las nuevas caracter\u00edsticas hasta la siguiente release. Feature-X: Cada nueva mejora o caracter\u00edstica que vayamos a introducir en nuestro software tendr\u00e1 una rama que contendr\u00e1 su desarrollo. Las ramas de feature salen de la rama develop y una vez completado el desarrollo de la mejora, se vuelven a integrar en el develop. Release-X: Las ramas de release se crean cuando se va a publicar la siguiente versi\u00f3n del software y surgen de la rama develop . En estas ramas, el desarrollo de nuevas caracter\u00edsticas se congela, y se trabaja en arreglar bugs y generar documentaci\u00f3n. Una vez listo para la publicaci\u00f3n, se integra en m\u00e1ster y se etiqueta con el n\u00famero de versi\u00f3n correspondiente. Se integran tambi\u00e9n con develop, ya que su contenido ha podido cambiar debido a nuevas mejoras. Hotfix-X: Si nuestro c\u00f3digo contiene bugs cr\u00edticos que es necesario parchear de manera inmediata, es posible crear una rama hotfix a partir de la publicaci\u00f3n correspondiente en la rama master. Esta rama contendr\u00e1 \u00fanicamente los cambios que haya que realizar para parchear el bug. Una vez arreglado, se integrar\u00e1 en master, con su etiqueta de versi\u00f3n correspondiente y en develop.","title":"Git Flow para los repos de Terraform"},{"location":"terraform/gitflow/#flujo-de-trabajo","text":"En este momento nosotros hacemos uso de un esquema reducido de esta metodolog\u00eda teniendo solamente de las ramas de caracter\u00edsticas, y ramas primarias. A continuaci\u00f3n hacemos una explicaci\u00f3n corta de c\u00f3mo aplicar la metodolog\u00eda.","title":"Flujo de trabajo"},{"location":"terraform/gitflow/#nota","text":"Se hace una documentaci\u00f3n por cada repositorio para hacerlo m\u00e1s sencillo.","title":"Nota"},{"location":"terraform/gitflow/gitflow-iac/","text":"Git Flow en el repo de IaC \u00b6 Preparar el entorno de desarrollo \u00b6 Antes de empezar a trabajar en una nueva caracter\u00edstica se debe tener limpio el repositorio local, con los \u00faltimos cambios del repositorio remoto,y estar en una nueva rama. Nota Recuerda que la rama principal de IaC es infraestructure-code Definir la rama principal, en este caso es infrastructure-code $ principal_branch = infrastructure-code Limpiar los cambios locales: Esto elimina todos los cambios que est\u00e1n por fuera del commit. $ git stash Ubicarse en la rama principal $ git checkout $principal_branch 2. Bajar los cambios remotos $ git pull origin $principal_branch 3. Crear una nueva rama para los cambios que se van a realizar. Nota: Tener en cuenta el est\u00e1ndar de nombramiento feature/{tribu}/{username}/{country}/{env}/{feature-description} $ branch_name = feature/SRE/rami802288/col/qa/firewall-paloalto $ git checkout -b $branch_name En este punto ya se pueden empezar a realizar los cambios o a crear la nueva funcionalidad, cuando los cambios hayan sido exitosos se procede al siguiente paso para integrarlos. Subir los cambios al repositorio \u00b6 Ver los cambios locales $ git status Agregar los cambios importantes al commit, use git add -A si se quieren agregar todos los cambios $ git add file1 folder1 ... Hacer un commit y subirlo $ git commit -m \"message description\" $ git push origin $branch_name Crear un pull requests y aprobarlo \u00b6 Se debe crear un pull requests de la nueva rama a la rama principal Este pull requests se comparte con los compa\u00f1eros de trabajo para que lo revisen y lo aprueben. Cuando haya sido aprobado se procede a integrar a la rama principal. Bajar el nuevo cambio de la rama principal \u00b6 Ubicarse en la rama principal git checkout $principal_branch Bajar los cambios remotos git pull origin $principal_branch Crear un tag de la nueva versi\u00f3n y subirlo al repositorio \u00b6 Esta no se est\u00e1 realizando en este repositorio por el momento","title":"Git Flow en el repo de IaC"},{"location":"terraform/gitflow/gitflow-iac/#git-flow-en-el-repo-de-iac","text":"","title":"Git Flow en el repo de IaC"},{"location":"terraform/gitflow/gitflow-iac/#preparar-el-entorno-de-desarrollo","text":"Antes de empezar a trabajar en una nueva caracter\u00edstica se debe tener limpio el repositorio local, con los \u00faltimos cambios del repositorio remoto,y estar en una nueva rama. Nota Recuerda que la rama principal de IaC es infraestructure-code Definir la rama principal, en este caso es infrastructure-code $ principal_branch = infrastructure-code Limpiar los cambios locales: Esto elimina todos los cambios que est\u00e1n por fuera del commit. $ git stash Ubicarse en la rama principal $ git checkout $principal_branch 2. Bajar los cambios remotos $ git pull origin $principal_branch 3. Crear una nueva rama para los cambios que se van a realizar. Nota: Tener en cuenta el est\u00e1ndar de nombramiento feature/{tribu}/{username}/{country}/{env}/{feature-description} $ branch_name = feature/SRE/rami802288/col/qa/firewall-paloalto $ git checkout -b $branch_name En este punto ya se pueden empezar a realizar los cambios o a crear la nueva funcionalidad, cuando los cambios hayan sido exitosos se procede al siguiente paso para integrarlos.","title":"Preparar el entorno de desarrollo"},{"location":"terraform/gitflow/gitflow-iac/#subir-los-cambios-al-repositorio","text":"Ver los cambios locales $ git status Agregar los cambios importantes al commit, use git add -A si se quieren agregar todos los cambios $ git add file1 folder1 ... Hacer un commit y subirlo $ git commit -m \"message description\" $ git push origin $branch_name","title":"Subir los cambios al repositorio"},{"location":"terraform/gitflow/gitflow-iac/#crear-un-pull-requests-y-aprobarlo","text":"Se debe crear un pull requests de la nueva rama a la rama principal Este pull requests se comparte con los compa\u00f1eros de trabajo para que lo revisen y lo aprueben. Cuando haya sido aprobado se procede a integrar a la rama principal.","title":"Crear un pull requests y aprobarlo"},{"location":"terraform/gitflow/gitflow-iac/#bajar-el-nuevo-cambio-de-la-rama-principal","text":"Ubicarse en la rama principal git checkout $principal_branch Bajar los cambios remotos git pull origin $principal_branch","title":"Bajar el nuevo cambio de la rama principal"},{"location":"terraform/gitflow/gitflow-iac/#crear-un-tag-de-la-nueva-version-y-subirlo-al-repositorio","text":"Esta no se est\u00e1 realizando en este repositorio por el momento","title":"Crear un tag de la nueva versi\u00f3n y subirlo al repositorio"},{"location":"terraform/gitflow/gitflow-infraestructure-modules/","text":"Git Flow en el repo de Infraestrucure Modules \u00b6 Preparar el entorno de desarrollo \u00b6 Antes de empezar a trabajar en una nueva caracteristica se debe tener limpio el repositorio local, con los ultimos cambios del repositorio remoto,y estar en una nueva rama. Nota Recuerda que la rama principal de Infraestructure-Modules es dev Definir la rama principal, en este caso es dev $ principal_branch = dev Limpiar los cambios locales: Esto elimina todos los cambios que esten por fuera del commit. $ git stash Ubicarse en la rama principal $ git checkout $principal_branch 2. Bajar los cambios remotos $ git pull origin $principal_branch 3. Crear una nueva rama para los cambios que se van a realizar. Nota : Tener en cuenta el estandar de nombramiento feature /{tribu}/{username}/{module}/ { feature - description } $ branch_name = feature/SRE/rami802288/aks_node_pools/some_change $ git checkout -b $branch_name En este punto ya se pueden empezar a realizar los cambios o a crear la nueva funcionalidad, cuando los cambios hayan sido exitosos se procede al siguiente paso para integrarlos. Subir los cambios al repositorio \u00b6 Ver los cambios locales $ git status Agregar los cambios importantes al commit, usar git add -A si se quieren agregar todos los cambios $ git add file1 folder1 ... Hacer un commit y subirlo $ git commit -m \"message description\" $ git push origin $branch_name Crear un pull requests y aprobarlo \u00b6 Se debe crear un pull requests de la nueva rama a la rama principal Este pull requests se comparte con los companeros de trabajo para que lo revisen y lo aprueben. Cuando haya sido aprobado se procede a integrar a la rama principal. Bajar el nuevo cambio de la rama principal \u00b6 Ubicarse en la rama principal git checkout $principal_branch Bajar los cambios remotos git pull origin $principal_branch Crear un tag de la nueva version y subirlo al repositorio \u00b6 Buscar el ultimo tag del modulo, en este ejemplo el modulo es aks_node_pools Definir el nombre del tag # tag_name={modulename}-{new_version} $ tag_name = aks_node_pools-v0.0.8 Crear el nuevo tag $ git tag -a tag_name -m \"message\" Subir el tag al repositorio remoto $ git pull origin tag_name Git Flow informatica@rami802288-11:~/dev/sre_meetings_directory$ ls ```bash start_symbol=$ informatica@rami802288-11:~/dev/sre_meetings_directory$ ls ```py def a(s): print(s)","title":"Git Flow en el repo de Infraestrucure Modules"},{"location":"terraform/gitflow/gitflow-infraestructure-modules/#git-flow-en-el-repo-de-infraestrucure-modules","text":"","title":"Git Flow en el repo de Infraestrucure Modules"},{"location":"terraform/gitflow/gitflow-infraestructure-modules/#preparar-el-entorno-de-desarrollo","text":"Antes de empezar a trabajar en una nueva caracteristica se debe tener limpio el repositorio local, con los ultimos cambios del repositorio remoto,y estar en una nueva rama. Nota Recuerda que la rama principal de Infraestructure-Modules es dev Definir la rama principal, en este caso es dev $ principal_branch = dev Limpiar los cambios locales: Esto elimina todos los cambios que esten por fuera del commit. $ git stash Ubicarse en la rama principal $ git checkout $principal_branch 2. Bajar los cambios remotos $ git pull origin $principal_branch 3. Crear una nueva rama para los cambios que se van a realizar. Nota : Tener en cuenta el estandar de nombramiento feature /{tribu}/{username}/{module}/ { feature - description } $ branch_name = feature/SRE/rami802288/aks_node_pools/some_change $ git checkout -b $branch_name En este punto ya se pueden empezar a realizar los cambios o a crear la nueva funcionalidad, cuando los cambios hayan sido exitosos se procede al siguiente paso para integrarlos.","title":"Preparar el entorno de desarrollo"},{"location":"terraform/gitflow/gitflow-infraestructure-modules/#subir-los-cambios-al-repositorio","text":"Ver los cambios locales $ git status Agregar los cambios importantes al commit, usar git add -A si se quieren agregar todos los cambios $ git add file1 folder1 ... Hacer un commit y subirlo $ git commit -m \"message description\" $ git push origin $branch_name","title":"Subir los cambios al repositorio"},{"location":"terraform/gitflow/gitflow-infraestructure-modules/#crear-un-pull-requests-y-aprobarlo","text":"Se debe crear un pull requests de la nueva rama a la rama principal Este pull requests se comparte con los companeros de trabajo para que lo revisen y lo aprueben. Cuando haya sido aprobado se procede a integrar a la rama principal.","title":"Crear un pull requests y aprobarlo"},{"location":"terraform/gitflow/gitflow-infraestructure-modules/#bajar-el-nuevo-cambio-de-la-rama-principal","text":"Ubicarse en la rama principal git checkout $principal_branch Bajar los cambios remotos git pull origin $principal_branch","title":"Bajar el nuevo cambio de la rama principal"},{"location":"terraform/gitflow/gitflow-infraestructure-modules/#crear-un-tag-de-la-nueva-version-y-subirlo-al-repositorio","text":"Buscar el ultimo tag del modulo, en este ejemplo el modulo es aks_node_pools Definir el nombre del tag # tag_name={modulename}-{new_version} $ tag_name = aks_node_pools-v0.0.8 Crear el nuevo tag $ git tag -a tag_name -m \"message\" Subir el tag al repositorio remoto $ git pull origin tag_name Git Flow informatica@rami802288-11:~/dev/sre_meetings_directory$ ls ```bash start_symbol=$ informatica@rami802288-11:~/dev/sre_meetings_directory$ ls ```py def a(s): print(s)","title":"Crear un tag de la nueva version y subirlo al repositorio"}]}